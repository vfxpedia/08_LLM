{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c753115",
   "metadata": {},
   "source": [
    "# 손글씨 숫자 예측 by MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d39bc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.61MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 149kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 1.66MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "# MNIST 데이터 로드\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_data = MNIST(root='./', train=True, download=True, transform=ToTensor())\n",
    "test_data = MNIST(root='./', train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afc62f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n",
      "torch.Size([60000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(test_data))\n",
    "print(train_data.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "084256d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.7-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.3-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.60.1-cp312-cp312-win_amd64.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.9-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from matplotlib) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.7-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 2.1/8.1 MB 122.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.3/8.1 MB 16.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 16.8 MB/s  0:00:00\n",
      "Using cached contourpy-1.3.3-cp312-cp312-win_amd64.whl (226 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.60.1-cp312-cp312-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.3/2.3 MB 64.7 MB/s  0:00:00\n",
      "Using cached kiwisolver-1.4.9-cp312-cp312-win_amd64.whl (73 kB)\n",
      "Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   -------------------------- ------------- 4/6 [contourpy]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   ---------------------------------------- 6/6 [matplotlib]\n",
      "\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.1 kiwisolver-1.4.9 matplotlib-3.10.7 pyparsing-3.2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae4c4759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(train_data.data[9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e9b71f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 처리를 위한 데이터 로더\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dafe2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # 강사님이 알려주신 설정\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91d3af00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "# model = MLP().to(device)\n",
    "model = MLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3378d3",
   "metadata": {},
   "source": [
    "### MLP(Multi-Layer Perceptron) 구조 완전 분석\n",
    "\n",
    "#### 🧠 Chain of Thought: 강사님의 설계 의도 파악\n",
    "\n",
    "##### 1단계: CNN vs MLP의 근본적인 차이\n",
    "\n",
    "**CNN과 비교하여 생각해봅시다:**\n",
    "\n",
    "| 특성 | CNN | MLP |\n",
    "|------|-----|-----|\n",
    "| **입력 형태** | 2D/3D 유지 `(1, 28, 28)` | 1D 벡터로 변환 `(784,)` |\n",
    "| **공간 정보** | 보존 (이웃 픽셀 관계 유지) | 손실 (모든 픽셀을 독립적으로 취급) |\n",
    "| **연산 방식** | 지역적(Convolution) | 전역적(Fully Connected) |\n",
    "| **파라미터 효율** | 높음 (가중치 공유) | 낮음 (모든 연결이 독립적) |\n",
    "\n",
    "**핵심 인사이트**: \n",
    "- MLP는 **공간 구조를 고려하지 않음** → 입력을 1차원 벡터로 펼쳐야 함\n",
    "- 모든 픽셀이 모든 뉴런에 연결됨 → **완전 연결(Fully Connected)**\n",
    "\n",
    "---\n",
    "\n",
    "##### 2단계: 왜 784 → 128 → 64 → 10 구조인가?\n",
    "\n",
    "```python\n",
    "self.fc1 = nn.Linear(784, 128)   # 입력층 → 은닉층1\n",
    "self.fc2 = nn.Linear(128, 64)    # 은닉층1 → 은닉층2\n",
    "self.fc3 = nn.Linear(64, 10)     # 은닉층2 → 출력층\n",
    "```\n",
    "\n",
    "**설계 철학: 점진적 차원 축소 (Feature Compression)**\n",
    "\n",
    "```\n",
    "784 → 128 → 64 → 10\n",
    " ↓     ↓     ↓    ↓\n",
    "원본  압축1 압축2 분류\n",
    "```\n",
    "\n",
    "이는 **정보의 계층적 압축(Hierarchical Compression)** 전략입니다.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔍 코드 상세 분석\n",
    "\n",
    "##### Step 1: 입력 차원이 왜 784인가?\n",
    "\n",
    "```python\n",
    "self.fc1 = nn.Linear(784, 128)\n",
    "```\n",
    "\n",
    "**계산 근거:**\n",
    "\n",
    "```\n",
    "MNIST 이미지 크기: 28 × 28 픽셀\n",
    "1차원 변환: 28 × 28 = 784\n",
    "\n",
    "시각화:\n",
    "[[ 0,  1,  2, ..., 27],     →  [0, 1, 2, ..., 783]\n",
    " [28, 29, 30, ..., 55],         (784개의 1차원 벡터)\n",
    " ...\n",
    " [756, 757, ..., 783]]\n",
    "```\n",
    "\n",
    "**왜 1차원으로 변환해야 하나?**\n",
    "\n",
    "MLP의 `nn.Linear`는 **행렬 곱셈(Matrix Multiplication)**을 수행합니다:\n",
    "\n",
    "```\n",
    "입력 벡터 (1 × 784) × 가중치 행렬 (784 × 128) = 출력 (1 × 128)\n",
    "```\n",
    "\n",
    "만약 2D 형태 `(28, 28)`을 그대로 입력하면:\n",
    "- 행렬 곱셈이 정의되지 않음\n",
    "- Linear 층은 마지막 차원만 변환 가능\n",
    "\n",
    "**따라서 반드시 평탄화(Flatten)가 필요합니다!**\n",
    "\n",
    "---\n",
    "\n",
    "##### Step 2: 왜 128개 뉴런인가?\n",
    "\n",
    "```python\n",
    "self.fc1 = nn.Linear(784, 128)  # 784 → 128로 약 1/6 압축\n",
    "```\n",
    "\n",
    "**설계 근거 (CoT):**\n",
    "\n",
    "1. **정보 압축**: 784개 픽셀 중 많은 부분이 배경(0)\n",
    "   - 실제 중요한 정보는 훨씬 적음\n",
    "   - 128차원으로 압축해도 충분히 표현 가능\n",
    "\n",
    "2. **계산 효율**: 784 → 10으로 직접 연결하지 않는 이유\n",
    "   - 너무 급격한 압축 → 정보 손실\n",
    "   - 중간 층이 점진적으로 추상화 수행\n",
    "\n",
    "3. **파라미터 개수**:\n",
    "```\n",
    "직접 연결 (784 → 10):\n",
    "파라미터 = 784 × 10 + 10 = 7,850개\n",
    "\n",
    "계층적 연결 (784 → 128 → 10):\n",
    "FC1 = 784 × 128 + 128 = 100,480\n",
    "FC2 = 128 × 10 + 10 = 1,290\n",
    "합계 = 101,770개\n",
    "```\n",
    "\n",
    "→ **더 많은 파라미터를 사용하지만, 표현력(Representation Power)이 훨씬 높음!**\n",
    "\n",
    "**128의 선택 기준:**\n",
    "- 2의 거듭제곱 근처 (64, 128, 256) → GPU 연산 효율적\n",
    "- 입력(784)과 출력(10)의 중간 스케일\n",
    "- 너무 크면: 과적합, 느림\n",
    "- 너무 작으면: 과소적합, 표현력 부족\n",
    "\n",
    "---\n",
    "\n",
    "##### Step 3: 왜 128 → 64로 또 압축하는가?\n",
    "\n",
    "```python\n",
    "self.fc2 = nn.Linear(128, 64)  # 128 → 64로 1/2 압축\n",
    "```\n",
    "\n",
    "**설계 근거 (CoT):**\n",
    "\n",
    "**계층적 특징 추출(Hierarchical Feature Learning)**\n",
    "\n",
    "```\n",
    "Layer 1 (784 → 128):  저수준 특징\n",
    "  - 엣지, 곡선, 기본 패턴 감지\n",
    "  - 예: \"여기에 수직선이 있다\", \"여기에 곡선이 있다\"\n",
    "\n",
    "Layer 2 (128 → 64):   중간 수준 특징\n",
    "  - 저수준 특징들을 조합\n",
    "  - 예: \"수직선 + 곡선 = 숫자 6의 일부\"\n",
    "\n",
    "Layer 3 (64 → 10):    고수준 특징 (분류)\n",
    "  - 최종 클래스 판단\n",
    "  - 예: \"이것은 숫자 6이다\"\n",
    "```\n",
    "\n",
    "**왜 한 번에 128 → 10으로 가지 않나?**\n",
    "\n",
    "```python\n",
    "# 옵션 1: 직접 연결 (얕은 네트워크)\n",
    "fc1: 784 → 128\n",
    "fc2: 128 → 10\n",
    "\n",
    "# 옵션 2: 계층적 연결 (깊은 네트워크) ← 강사님 선택\n",
    "fc1: 784 → 128\n",
    "fc2: 128 → 64\n",
    "fc3: 64 → 10\n",
    "```\n",
    "\n",
    "**깊은 네트워크의 장점:**\n",
    "1. **더 복잡한 패턴 학습** 가능\n",
    "2. **점진적 추상화** → 안정적 학습\n",
    "3. **Gradient Flow** 개선 (ReLU와 함께 사용 시)\n",
    "\n",
    "---\n",
    "\n",
    "##### Step 4: ReLU 활성화 함수의 역할\n",
    "\n",
    "```python\n",
    "self.relu1 = nn.ReLU()\n",
    "self.relu2 = nn.ReLU()\n",
    "```\n",
    "\n",
    "**왜 각 층마다 ReLU를 넣는가?**\n",
    "\n",
    "**비선형성(Non-linearity) 추가**\n",
    "\n",
    "ReLU가 없다면:\n",
    "```python\n",
    "# ReLU 없이\n",
    "x = fc1(x)      # W1 * x + b1\n",
    "x = fc2(x)      # W2 * (W1 * x + b1) + b2\n",
    "x = fc3(x)      # W3 * (W2 * (W1 * x + b1) + b2) + b3\n",
    "\n",
    "# 수학적으로 단순화하면:\n",
    "x = W_combined * x + b_combined\n",
    "\n",
    "→ 결국 1개의 선형 층과 동일!\n",
    "→ 깊게 쌓는 의미가 없음\n",
    "```\n",
    "\n",
    "**ReLU가 있으면:**\n",
    "```python\n",
    "x = relu(fc1(x))    # max(0, W1 * x + b1)\n",
    "x = relu(fc2(x))    # max(0, W2 * max(...))\n",
    "x = fc3(x)\n",
    "\n",
    "→ 비선형 변환 → 복잡한 패턴 학습 가능\n",
    "```\n",
    "\n",
    "**ReLU의 특징:**\n",
    "- `ReLU(x) = max(0, x)`\n",
    "- 음수는 0으로, 양수는 그대로\n",
    "- 계산이 매우 빠름\n",
    "- Gradient Vanishing 문제 완화\n",
    "\n",
    "**왜 마지막 fc3 뒤에는 ReLU가 없나?**\n",
    "```python\n",
    "self.fc3 = nn.Linear(64, 10)  # ReLU 없음!\n",
    "```\n",
    "\n",
    "→ 출력층은 **10개 클래스의 점수(logits)** 를 출력해야 함\n",
    "→ 음수 값도 필요 (어떤 클래스가 아닐 확률 표현)\n",
    "→ 이후 `CrossEntropyLoss`가 내부적으로 Softmax 적용\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔄 Forward Pass 동작 흐름\n",
    "\n",
    "##### 전체 데이터 흐름\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    x = x.view(x.size(0), -1)        # Step 1: 평탄화\n",
    "    x = self.relu1(self.fc1(x))      # Step 2: FC1 + ReLU\n",
    "    x = self.relu2(self.fc2(x))      # Step 3: FC2 + ReLU\n",
    "    return self.fc3(x)               # Step 4: FC3 (출력)\n",
    "```\n",
    "\n",
    "##### Step-by-Step 상세 분석\n",
    "\n",
    "###### **Step 1: 평탄화 (Flatten)**\n",
    "\n",
    "```python\n",
    "x = x.view(x.size(0), -1)\n",
    "```\n",
    "\n",
    "**동작 분석:**\n",
    "\n",
    "```python\n",
    "# 입력 형태 (예: batch_size=32)\n",
    "x.shape: (32, 1, 28, 28)\n",
    "         ↑   ↑   ↑   ↑\n",
    "       batch ch  H   W\n",
    "\n",
    "x.size(0) = 32  # 배치 크기\n",
    "-1 = 자동 계산   # 나머지 차원을 하나로 합침\n",
    "\n",
    "# 출력 형태\n",
    "x.shape: (32, 784)\n",
    "         ↑    ↑\n",
    "       batch 1×28×28=784\n",
    "```\n",
    "\n",
    "**`view(-1)` vs `view(x.size(0), -1)` 비교:**\n",
    "\n",
    "```python\n",
    "# 잘못된 방법\n",
    "x = x.view(-1)  \n",
    "# 결과: (32×784=25088,) → 배치 구분이 사라짐! ❌\n",
    "\n",
    "# 올바른 방법\n",
    "x = x.view(x.size(0), -1)\n",
    "# 결과: (32, 784) → 배치 유지 ✓\n",
    "```\n",
    "\n",
    "**왜 이렇게 해야 하나?**\n",
    "- `x.size(0)`: 배치 차원을 명시적으로 보존\n",
    "- `-1`: 나머지 차원은 자동 계산 (1×28×28 → 784)\n",
    "- 배치 단위 병렬 처리 유지\n",
    "\n",
    "---\n",
    "\n",
    "###### **Step 2: 첫 번째 은닉층**\n",
    "\n",
    "```python\n",
    "x = self.relu1(self.fc1(x))\n",
    "```\n",
    "\n",
    "**단계별 분해:**\n",
    "\n",
    "```python\n",
    "# fc1 적용\n",
    "z1 = self.fc1(x)  # (32, 784) × (784, 128) → (32, 128)\n",
    "                   # 가중치: 100,352개 (784×128 + 128)\n",
    "\n",
    "# relu1 적용\n",
    "x = self.relu1(z1)  # max(0, z1)\n",
    "```\n",
    "\n",
    "**내부 동작:**\n",
    "\n",
    "```\n",
    "입력: [0.5, -0.3, 0.8, -1.2, ...]  (784개 값)\n",
    "      ↓ (가중치 행렬 곱셈)\n",
    "중간: [2.1, -0.5, 1.3, -0.8, ...]  (128개 값)\n",
    "      ↓ (ReLU 적용)\n",
    "출력: [2.1,  0.0, 1.3,  0.0, ...]  (128개 값, 음수→0)\n",
    "```\n",
    "\n",
    "**의미:**\n",
    "- 784개 픽셀 정보를 128개 특징으로 압축\n",
    "- 음수(비활성) 뉴런 제거 → 희소성(Sparsity)\n",
    "- 다음 층에 중요한 특징만 전달\n",
    "\n",
    "---\n",
    "\n",
    "###### **Step 3: 두 번째 은닉층**\n",
    "\n",
    "```python\n",
    "x = self.relu2(self.fc2(x))\n",
    "```\n",
    "\n",
    "**단계별 분해:**\n",
    "\n",
    "```python\n",
    "# fc2 적용\n",
    "z2 = self.fc2(x)  # (32, 128) × (128, 64) → (32, 64)\n",
    "                   # 가중치: 8,256개 (128×64 + 64)\n",
    "\n",
    "# relu2 적용\n",
    "x = self.relu2(z2)  # max(0, z2)\n",
    "```\n",
    "\n",
    "**추상화 수준 증가:**\n",
    "\n",
    "```\n",
    "Layer 1 (128개 특징):\n",
    "  - \"여기 수직선\", \"여기 곡선\", \"여기 교차점\" ...\n",
    "\n",
    "Layer 2 (64개 특징):\n",
    "  - \"수직선 + 곡선 조합\", \"특정 형태 패턴\" ...\n",
    "  - 더 고수준의 의미 추출\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###### **Step 4: 출력층**\n",
    "\n",
    "```python\n",
    "return self.fc3(x)\n",
    "```\n",
    "\n",
    "**최종 변환:**\n",
    "\n",
    "```python\n",
    "output = self.fc3(x)  # (32, 64) × (64, 10) → (32, 10)\n",
    "                       # 가중치: 650개 (64×10 + 10)\n",
    "```\n",
    "\n",
    "**출력 형태:**\n",
    "\n",
    "```python\n",
    "# 예시 출력 (1개 샘플)\n",
    "tensor([[-2.3,  0.5,  1.2,  3.5, -0.8,  0.2, -1.5,  2.1, -0.3,  0.9]])\n",
    "          ↑     ↑     ↑     ↑     ↑     ↑     ↑     ↑     ↑     ↑\n",
    "         클래스0 1    2     3     4     5     6     7     8     9\n",
    "\n",
    "# 가장 큰 값: 3.5 (클래스 3)\n",
    "# 예측: 이 이미지는 숫자 \"3\"\n",
    "```\n",
    "\n",
    "**ReLU를 적용하지 않는 이유:**\n",
    "- 음수 값도 의미가 있음 (낮은 확률/신뢰도)\n",
    "- Loss 함수(CrossEntropy)가 logits를 받아 Softmax 처리\n",
    "- 출력 범위 제한하면 학습 불안정\n",
    "\n",
    "---\n",
    "\n",
    "#### 📊 전체 구조 비교표\n",
    "\n",
    "##### MLP vs CNN 비교\n",
    "\n",
    "```python\n",
    "# MLP (이번 코드)\n",
    "Input (28×28) → Flatten(784) → FC(128) → FC(64) → FC(10)\n",
    "\n",
    "# CNN (이전 코드)\n",
    "Input (28×28) → Conv(32) → Conv(64) → Pool → Flatten(10816) → FC(100) → FC(10)\n",
    "```\n",
    "\n",
    "| 구조 요소 | MLP | CNN |\n",
    "|-----------|-----|-----|\n",
    "| **초기 처리** | 즉시 Flatten | 공간 구조 유지 |\n",
    "| **특징 추출** | Fully Connected | Convolution |\n",
    "| **파라미터 수** | 많음 | 적음 (가중치 공유) |\n",
    "| **공간 정보** | 손실 | 보존 |\n",
    "| **성능 (이미지)** | 낮음 | 높음 |\n",
    "| **학습 속도** | 빠름 | 느림 |\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔢 파라미터 계산 상세\n",
    "\n",
    "##### 전체 파라미터 분석\n",
    "\n",
    "```python\n",
    "# FC1\n",
    "가중치: 784 × 128 = 100,352\n",
    "편향:   128\n",
    "소계:   100,480개\n",
    "\n",
    "# FC2\n",
    "가중치: 128 × 64 = 8,192\n",
    "편향:   64\n",
    "소계:   8,256개\n",
    "\n",
    "# FC3\n",
    "가중치: 64 × 10 = 640\n",
    "편향:   10\n",
    "소계:   650개\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━\n",
    "총 파라미터: 109,386개\n",
    "```\n",
    "\n",
    "**CNN과 비교:**\n",
    "- CNN: 1,101,526개 (MLP의 약 10배)\n",
    "- MLP: 109,386개\n",
    "\n",
    "**왜 이렇게 차이가 나는가?**\n",
    "```\n",
    "CNN의 fc1: 10,816 → 100 (1,081,700개 파라미터)\n",
    "MLP의 fc1: 784 → 128 (100,480개 파라미터)\n",
    "\n",
    "→ Flatten 후 크기가 CNN이 훨씬 큼 (10,816 vs 784)\n",
    "→ CNN은 Conv로 채널이 증가했기 때문\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 💡 설계 철학 정리\n",
    "\n",
    "##### 강사님이 이 구조를 선택한 이유\n",
    "\n",
    "###### **1. 교육적 목적**\n",
    "```\n",
    "단순 → 복잡 순서로 학습\n",
    "MLP (기본) → CNN (심화)\n",
    "```\n",
    "\n",
    "###### **2. 점진적 압축 전략**\n",
    "```\n",
    "784 → 128 (약 1/6 압축)\n",
    "128 → 64  (1/2 압축)\n",
    "64 → 10   (클래스 분류)\n",
    "\n",
    "급격한 압축보다 점진적 압축이 안정적\n",
    "```\n",
    "\n",
    "###### **3. 적절한 은닉층 크기**\n",
    "- **128**: 충분한 표현력, GPU 효율적\n",
    "- **64**: 추가 추상화, 과적합 방지\n",
    "- **2개 은닉층**: 얕지도 깊지도 않은 적정 깊이\n",
    "\n",
    "###### **4. ReLU 배치**\n",
    "- 각 은닉층 후: 비선형성 추가\n",
    "- 출력층 전: 없음 (logits 출력)\n",
    "\n",
    "---\n",
    "\n",
    "#### 🎯 핵심 개념 요약\n",
    "\n",
    "##### ✅ 반드시 기억할 것\n",
    "\n",
    "###### **1. MLP는 1D 입력만 받음**\n",
    "```python\n",
    "x = x.view(x.size(0), -1)  # (B, 1, 28, 28) → (B, 784)\n",
    "```\n",
    "→ 공간 구조 무시, 모든 픽셀을 독립적으로 취급\n",
    "\n",
    "###### **2. 계층적 차원 축소**\n",
    "```\n",
    "784 → 128 → 64 → 10\n",
    "원본    저수준   중간   분류\n",
    "```\n",
    "→ 각 층이 더 추상적인 특징 학습\n",
    "\n",
    "###### **3. ReLU는 비선형성 제공**\n",
    "```\n",
    "선형층만: f(f(x)) = f'(x) (여전히 선형)\n",
    "ReLU 추가: 복잡한 비선형 패턴 학습 가능\n",
    "```\n",
    "\n",
    "###### **4. 출력층에는 활성화 함수 없음**\n",
    "```\n",
    "FC3 → 직접 출력 (logits)\n",
    "→ CrossEntropyLoss가 Softmax 처리\n",
    "```\n",
    "\n",
    "###### **5. 배치 차원 보존**\n",
    "```python\n",
    "x.view(x.size(0), -1)  # 배치 유지 ✓\n",
    "x.view(-1)             # 배치 손실 ❌\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🚀 실전 활용 팁\n",
    "\n",
    "##### MLP를 개선하려면?\n",
    "\n",
    "###### **1. 더 깊게 만들기**\n",
    "```python\n",
    "fc1: 784 → 256\n",
    "fc2: 256 → 128\n",
    "fc3: 128 → 64\n",
    "fc4: 64 → 10\n",
    "```\n",
    "\n",
    "###### **2. Dropout 추가 (과적합 방지)**\n",
    "```python\n",
    "self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "def forward(self, x):\n",
    "    x = self.dropout(self.relu1(self.fc1(x)))\n",
    "    x = self.dropout(self.relu2(self.fc2(x)))\n",
    "    return self.fc3(x)\n",
    "```\n",
    "\n",
    "###### **3. Batch Normalization**\n",
    "```python\n",
    "self.bn1 = nn.BatchNorm1d(128)\n",
    "self.bn2 = nn.BatchNorm1d(64)\n",
    "```\n",
    "\n",
    "###### **4. 다른 활성화 함수**\n",
    "```python\n",
    "self.relu1 = nn.LeakyReLU()  # 음수도 약간 통과\n",
    "self.relu2 = nn.ELU()        # 음수에서 부드러운 곡선\n",
    "```\n",
    "\n",
    "하지만 **이미지에는 CNN이 훨씬 효과적**이라는 것을 기억하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "756d0e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26dc96b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6891541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | loss: 0.0693\n",
      "Epoch 2 | loss: 0.1046\n",
      "Epoch 3 | loss: 0.1928\n",
      "Epoch 4 | loss: 0.0117\n",
      "Epoch 5 | loss: 0.0163\n",
      "Epoch 6 | loss: 0.0745\n",
      "Epoch 7 | loss: 0.0281\n",
      "Epoch 8 | loss: 0.0255\n",
      "Epoch 9 | loss: 0.0077\n",
      "Epoch 10 | loss: 0.0101\n",
      "Epoch 11 | loss: 0.0043\n",
      "Epoch 12 | loss: 0.0054\n",
      "Epoch 13 | loss: 0.0017\n",
      "Epoch 14 | loss: 0.0313\n",
      "Epoch 15 | loss: 0.0015\n",
      "Epoch 16 | loss: 0.0008\n",
      "Epoch 17 | loss: 0.0309\n",
      "Epoch 18 | loss: 0.0013\n",
      "Epoch 19 | loss: 0.1209\n",
      "Epoch 20 | loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optim = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(20):\n",
    "    for data, label in train_loader:\n",
    "        optim.zero_grad()\n",
    "        data = torch.reshape(data, (-1, 784)).to(device)\n",
    "        preds = model(data)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(preds, label.to(device))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ea4e1",
   "metadata": {},
   "source": [
    "### PyTorch 학습 루프 완전 분석: Optimizer와 학습 과정\n",
    "\n",
    "#### 🧠 Chain of Thought: 학습의 본질 이해하기\n",
    "\n",
    "##### 근본적인 질문: 딥러닝 학습이란 무엇인가?\n",
    "\n",
    "```\n",
    "초기 상태: 랜덤 가중치 → 엉터리 예측\n",
    "       ↓\n",
    "    학습 과정 (반복)\n",
    "       ↓\n",
    "최종 상태: 최적화된 가중치 → 정확한 예측\n",
    "```\n",
    "\n",
    "**핵심**: \n",
    "- **학습 = 가중치(Weight) 업데이트 과정**\n",
    "- **목표**: Loss(손실)를 최소화하는 가중치 찾기\n",
    "- **방법**: Optimizer(최적화 알고리즘) 사용\n",
    "\n",
    "---\n",
    "\n",
    "#### 📚 Optimizer(옵티마이저)란?\n",
    "\n",
    "##### 개념 정의\n",
    "\n",
    "```python\n",
    "from torch.optim.adam import Adam\n",
    "```\n",
    "\n",
    "**Optimizer(최적화기)**: \n",
    "- 모델의 가중치를 **어떻게(How)** 업데이트할지 결정하는 알고리즘\n",
    "- Loss를 최소화하는 방향으로 파라미터를 조정\n",
    "- 경사하강법(Gradient Descent)의 발전된 형태\n",
    "\n",
    "##### 왜 'optim'이라는 이름을 쓰는가?\n",
    "\n",
    "```python\n",
    "optim = Adam(model.parameters(), lr=learning_rate)\n",
    "#  ↑\n",
    "# optimization의 줄임말\n",
    "# 관례적으로 optimizer 객체를 'optim'이라는 변수명으로 저장\n",
    "```\n",
    "\n",
    "**비유**:\n",
    "- 모델 = 자동차\n",
    "- Loss = 목적지까지의 거리\n",
    "- Optimizer = 내비게이션 (어느 방향으로 갈지 안내)\n",
    "- Learning Rate = 자동차 속도\n",
    "\n",
    "---\n",
    "\n",
    "#### 🚀 Adam Optimizer란?\n",
    "\n",
    "##### Adam = **Ada**ptive **M**oment Estimation\n",
    "\n",
    "```python\n",
    "from torch.optim.adam import Adam\n",
    "optim = Adam(model.parameters(), lr=learning_rate)\n",
    "```\n",
    "\n",
    "##### Adam의 위치: Optimizer 계보도\n",
    "\n",
    "```\n",
    "Gradient Descent (1847)\n",
    "    ↓\n",
    "SGD (Stochastic Gradient Descent)\n",
    "    ↓\n",
    "    ├─ Momentum (관성 추가)\n",
    "    ├─ RMSprop (학습률 조정)\n",
    "    └─ Adam (2014) ← 가장 많이 사용 ⭐\n",
    "         ↓\n",
    "    AdamW (가중치 감쇠 개선)\n",
    "```\n",
    "\n",
    "##### Adam이 특별한 이유\n",
    "\n",
    "###### **1. 적응형 학습률 (Adaptive Learning Rate)**\n",
    "\n",
    "```\n",
    "일반 SGD: 모든 파라미터에 동일한 학습률\n",
    "   W1 += -0.001 × gradient1\n",
    "   W2 += -0.001 × gradient2\n",
    "   W3 += -0.001 × gradient3\n",
    "\n",
    "Adam: 각 파라미터마다 다른 학습률\n",
    "   W1 += -0.003 × gradient1  (빠르게 변화)\n",
    "   W2 += -0.0001 × gradient2  (천천히 변화)\n",
    "   W3 += -0.002 × gradient3\n",
    "```\n",
    "\n",
    "###### **2. 모멘텀(Momentum) 효과**\n",
    "\n",
    "```\n",
    "SGD: 현재 기울기만 사용\n",
    "   ↓\n",
    "  ↙ ↓ ↘\n",
    " 현재 방향으로만 이동 (불안정)\n",
    "\n",
    "Adam: 과거 기울기 정보도 활용\n",
    "   ↓\n",
    "  ↓↓↓\n",
    " 관성을 가지고 안정적으로 이동\n",
    "```\n",
    "\n",
    "###### **3. 자동 조정**\n",
    "\n",
    "- 기울기가 큰 곳: 학습률 자동 감소 (과도한 변화 방지)\n",
    "- 기울기가 작은 곳: 학습률 자동 증가 (학습 가속)\n",
    "\n",
    "##### 왜 강사님이 Adam을 선택했는가?\n",
    "\n",
    "| 이유 | 설명 |\n",
    "|------|------|\n",
    "| **범용성** | 대부분의 문제에서 잘 작동 |\n",
    "| **안정성** | 학습률 튜닝이 덜 민감 |\n",
    "| **빠른 수렴** | 적응형 학습률로 효율적 |\n",
    "| **기본 선택** | 딥러닝 커뮤니티 표준 |\n",
    "| **초보자 친화적** | 하이퍼파라미터 조정 부담 적음 |\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔢 Learning Rate: 왜 1e-3인가?\n",
    "\n",
    "##### 1e-3의 의미\n",
    "\n",
    "```python\n",
    "learning_rate = 1e-3\n",
    "# 1e-3 = 1 × 10^(-3) = 0.001\n",
    "```\n",
    "\n",
    "##### CoT: Learning Rate가 너무 크면 안 되는 이유\n",
    "\n",
    "###### **시나리오 1: Learning Rate가 너무 클 때 (예: 1.0)**\n",
    "\n",
    "```\n",
    "현재 Loss: 2.5\n",
    "최적 가중치: 3.0\n",
    "현재 가중치: 2.0\n",
    "\n",
    "업데이트:\n",
    "W_new = W_old - lr × gradient\n",
    "W_new = 2.0 - 1.0 × 1.0 = 1.0  (너무 많이 이동!)\n",
    "\n",
    "다음 반복:\n",
    "W_new = 1.0 - 1.0 × (-2.0) = 3.0  (최적값 통과)\n",
    "\n",
    "다음 반복:\n",
    "W_new = 3.0 - 1.0 × 2.0 = 1.0  (다시 멀어짐)\n",
    "\n",
    "결과: 진동하며 수렴 실패! ❌\n",
    "```\n",
    "\n",
    "**시각화:**\n",
    "```\n",
    "Loss\n",
    " │\n",
    " │    *           *\n",
    " │       *     *\n",
    " │         * *\n",
    " │─────────────────── Weight\n",
    "      진동하며 발산\n",
    "```\n",
    "\n",
    "###### **시나리오 2: Learning Rate가 적절할 때 (예: 0.001)**\n",
    "\n",
    "```\n",
    "현재 가중치: 2.0\n",
    "최적 가중치: 3.0\n",
    "\n",
    "업데이트:\n",
    "W = 2.0 - 0.001 × 1.0 = 1.999  (조금 이동)\n",
    "W = 1.999 - 0.001 × 0.99 = 1.99901\n",
    "...\n",
    "점진적으로 3.0에 수렴 ✓\n",
    "```\n",
    "\n",
    "**시각화:**\n",
    "```\n",
    "Loss\n",
    " │ \\\n",
    " │  \\___\n",
    " │      \\___\n",
    " │          \\______\n",
    " │─────────────────── Weight\n",
    "    부드럽게 수렴\n",
    "```\n",
    "\n",
    "###### **시나리오 3: Learning Rate가 너무 작을 때 (예: 1e-7)**\n",
    "\n",
    "```\n",
    "W = 2.0 - 0.0000001 × 1.0 = 1.9999999\n",
    "W = 1.9999999 - 0.0000001 × 1.0 = 1.9999998\n",
    "...\n",
    "\n",
    "문제: \n",
    "- 100만 번 반복해도 거의 변화 없음\n",
    "- 학습 시간이 너무 오래 걸림\n",
    "- 실용적이지 않음\n",
    "```\n",
    "\n",
    "##### Learning Rate 비교표\n",
    "\n",
    "| Learning Rate | 효과 | 문제점 |\n",
    "|--------------|------|--------|\n",
    "| **1.0** | 매우 큰 변화 | 발산, 진동 |\n",
    "| **0.1** | 큰 변화 | 불안정, 최적값 통과 |\n",
    "| **0.01** | 적당한 변화 | 약간 불안정 |\n",
    "| **0.001 (1e-3)** ⭐ | 안정적 수렴 | 균형 잡힘 |\n",
    "| **0.0001 (1e-4)** | 매우 안정적 | 느린 학습 |\n",
    "| **0.00001 (1e-5)** | 거의 변화 없음 | 너무 느림 |\n",
    "\n",
    "##### 왜 1e-3이 표준인가?\n",
    "\n",
    "**경험적 발견 (Empirical Finding):**\n",
    "\n",
    "```python\n",
    "# 딥러닝 커뮤니티의 경험적 권장값\n",
    "SGD:      lr = 0.01 ~ 0.1\n",
    "Adam:     lr = 0.001 (1e-3)  ← 가장 보편적 ⭐\n",
    "AdamW:    lr = 0.0001 ~ 0.001\n",
    "RMSprop:  lr = 0.001\n",
    "```\n",
    "\n",
    "**1e-3이 좋은 이유:**\n",
    "1. **안정적 수렴**: 너무 빠르지도, 느리지도 않음\n",
    "2. **Adam과 궁합**: Adam의 적응형 학습률과 조화\n",
    "3. **범용성**: 대부분의 문제에서 좋은 출발점\n",
    "4. **검증됨**: 수많은 논문과 실험에서 입증\n",
    "\n",
    "---\n",
    "\n",
    "#### 📝 코드 라인별 상세 분석\n",
    "\n",
    "##### Line 1-2: Optimizer Import\n",
    "\n",
    "```python\n",
    "from torch.optim.adam import Adam\n",
    "```\n",
    "\n",
    "**의미:**\n",
    "- `torch.optim`: PyTorch의 최적화 모듈\n",
    "- `.adam`: Adam optimizer 서브모듈\n",
    "- `Adam`: Adam 클래스 import\n",
    "\n",
    "**대안:**\n",
    "```python\n",
    "# 방법 1 (강사님 방식)\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "# 방법 2 (더 일반적)\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(...)\n",
    "\n",
    "# 방법 3\n",
    "from torch import optim\n",
    "optimizer = optim.Adam(...)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### Line 4-5: Optimizer 생성\n",
    "\n",
    "```python\n",
    "learning_rate = 1e-3\n",
    "optim = Adam(model.parameters(), lr=learning_rate)\n",
    "```\n",
    "\n",
    "###### **`model.parameters()`의 의미**\n",
    "\n",
    "```python\n",
    "# model.parameters()는 Generator 객체\n",
    "# 모델의 모든 학습 가능한 파라미터를 반환\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n",
    "\n",
    "# 출력:\n",
    "# fc1.weight: torch.Size([128, 784])\n",
    "# fc1.bias: torch.Size([128])\n",
    "# fc2.weight: torch.Size([64, 128])\n",
    "# fc2.bias: torch.Size([64])\n",
    "# fc3.weight: torch.Size([10, 64])\n",
    "# fc3.bias: torch.Size([10])\n",
    "```\n",
    "\n",
    "**왜 `model.parameters()`를 전달하는가?**\n",
    "\n",
    "→ Optimizer가 **어떤 가중치를 업데이트할지** 알아야 하기 때문\n",
    "\n",
    "```python\n",
    "optim = Adam(model.parameters(), lr=learning_rate)\n",
    "              ↑\n",
    "         optimizer가 관리할 파라미터 목록\n",
    "```\n",
    "\n",
    "###### **`lr=learning_rate`의 의미**\n",
    "\n",
    "```python\n",
    "lr=learning_rate  # lr = learning rate\n",
    "lr=1e-3          # 0.001의 과학적 표기법\n",
    "```\n",
    "\n",
    "**Adam의 다른 파라미터 (선택사항):**\n",
    "\n",
    "```python\n",
    "optim = Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,              # 학습률\n",
    "    betas=(0.9, 0.999),   # 모멘텀 계수 (기본값)\n",
    "    eps=1e-8,             # 수치 안정성 (기본값)\n",
    "    weight_decay=0        # L2 정규화 (기본값)\n",
    ")\n",
    "```\n",
    "\n",
    "강사님은 기본값이 이미 좋기 때문에 `lr`만 명시!\n",
    "\n",
    "---\n",
    "\n",
    "##### Line 7-17: 학습 루프 (Training Loop)\n",
    "\n",
    "```python\n",
    "for epoch in range(20):\n",
    "    for data, label in train_loader:\n",
    "        optim.zero_grad()\n",
    "        data = torch.reshape(data, (-1, 784)).to(device)\n",
    "        preds = model(data)\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss()(preds, label.to(device))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} | loss: {loss.item():.4f}\")\n",
    "```\n",
    "\n",
    "##### Step-by-Step 분석\n",
    "\n",
    "###### **Line 7: Epoch 반복**\n",
    "\n",
    "```python\n",
    "for epoch in range(20):\n",
    "```\n",
    "\n",
    "**Epoch란?**\n",
    "- **1 Epoch** = 전체 훈련 데이터를 한 번 모두 본 것\n",
    "- 20 Epochs = 전체 데이터를 20번 반복 학습\n",
    "\n",
    "**왜 여러 번 반복하는가?**\n",
    "\n",
    "```\n",
    "1 Epoch:  데이터 → 학습 → 조금 개선\n",
    "2 Epoch:  같은 데이터 → 학습 → 더 개선\n",
    "3 Epoch:  같은 데이터 → 학습 → 더 개선\n",
    "...\n",
    "20 Epoch: 충분히 학습됨\n",
    "```\n",
    "\n",
    "**시각화:**\n",
    "```\n",
    "Loss\n",
    " │ \\\n",
    " │  \\___      Epoch 1-5: 빠른 감소\n",
    " │      \\___  Epoch 6-15: 점진적 감소\n",
    " │          \\____ Epoch 16-20: 수렴\n",
    " │─────────────────── Epoch\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###### **Line 8: Batch 반복**\n",
    "\n",
    "```python\n",
    "for data, label in train_loader:\n",
    "```\n",
    "\n",
    "**Batch란?**\n",
    "- 전체 데이터를 작은 묶음으로 나눈 것\n",
    "- 예: 60,000개 데이터 → 32개씩 묶음 → 1,875 batches\n",
    "\n",
    "**`train_loader`의 역할:**\n",
    "\n",
    "```python\n",
    "# train_loader 예시 (batch_size=32)\n",
    "# Batch 1: data.shape = (32, 1, 28, 28), label.shape = (32,)\n",
    "# Batch 2: data.shape = (32, 1, 28, 28), label.shape = (32,)\n",
    "# ...\n",
    "# Batch 1875: data.shape = (32, 1, 28, 28), label.shape = (32,)\n",
    "```\n",
    "\n",
    "**왜 Batch로 나누는가?**\n",
    "\n",
    "| 방식 | 장점 | 단점 |\n",
    "|------|------|------|\n",
    "| **전체 데이터 한번에** | 정확한 기울기 | 메모리 부족, 느림 |\n",
    "| **1개씩** | 빠름, 메모리 효율 | 불안정, 노이즈 많음 |\n",
    "| **Batch (32~128개)** ⭐ | 균형 잡힘 | 최적의 선택 |\n",
    "\n",
    "---\n",
    "\n",
    "###### **Line 9: Gradient 초기화**\n",
    "\n",
    "```python\n",
    "optim.zero_grad()\n",
    "```\n",
    "\n",
    "**왜 필요한가?**\n",
    "\n",
    "**PyTorch의 특징: Gradient 자동 누적**\n",
    "\n",
    "```python\n",
    "# 첫 번째 batch\n",
    "loss1.backward()  # gradient 계산 → 저장\n",
    "\n",
    "# 두 번째 batch (zero_grad() 없으면)\n",
    "loss2.backward()  # gradient 계산 → 이전 gradient에 더해짐! ❌\n",
    "\n",
    "# 결과: gradient = grad1 + grad2 (잘못된 값)\n",
    "```\n",
    "\n",
    "**올바른 방법:**\n",
    "\n",
    "```python\n",
    "# Batch 1\n",
    "optim.zero_grad()    # gradient 초기화\n",
    "loss1.backward()     # gradient 계산\n",
    "optim.step()         # 업데이트\n",
    "\n",
    "# Batch 2\n",
    "optim.zero_grad()    # gradient 다시 초기화 ✓\n",
    "loss2.backward()     # 새로운 gradient 계산\n",
    "optim.step()         # 업데이트\n",
    "```\n",
    "\n",
    "**내부 동작:**\n",
    "\n",
    "```python\n",
    "# optim.zero_grad() 실행 전\n",
    "fc1.weight.grad = tensor([[0.5, -0.3, ...]])  # 이전 값\n",
    "\n",
    "# optim.zero_grad() 실행 후\n",
    "fc1.weight.grad = tensor([[0.0, 0.0, ...]])   # 초기화됨\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###### **Line 10: 데이터 변환 및 GPU 전송**\n",
    "\n",
    "```python\n",
    "data = torch.reshape(data, (-1, 784)).to(device)\n",
    "```\n",
    "\n",
    "**두 가지 작업을 동시에:**\n",
    "\n",
    "####### **1. `torch.reshape(data, (-1, 784))`**\n",
    "\n",
    "```python\n",
    "# 입력 형태\n",
    "data.shape: (32, 1, 28, 28)\n",
    "             ↑   ↑   ↑   ↑\n",
    "          batch ch  H   W\n",
    "\n",
    "# 출력 형태\n",
    "reshaped.shape: (32, 784)\n",
    "                 ↑    ↑\n",
    "              batch  flattened\n",
    "```\n",
    "\n",
    "**`-1`의 의미:**\n",
    "```python\n",
    "-1 = 자동 계산\n",
    "32 × 1 × 28 × 28 = 25,088개 원소\n",
    "25,088 / 784 = 32\n",
    "\n",
    "따라서 (-1, 784) = (32, 784)\n",
    "```\n",
    "\n",
    "**왜 reshape가 필요한가?**\n",
    "→ MLP는 1D 입력만 받기 때문 (이전 설명 참조)\n",
    "\n",
    "####### **2. `.to(device)`**\n",
    "\n",
    "```python\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# GPU 사용 가능하면\n",
    "data.to(device)  # → GPU로 이동\n",
    "\n",
    "# GPU 없으면\n",
    "data.to(device)  # → CPU에 그대로\n",
    "```\n",
    "\n",
    "**왜 필요한가?**\n",
    "- 모델이 GPU에 있으면 데이터도 GPU에 있어야 함\n",
    "- CPU 데이터 × GPU 모델 = 오류! ❌\n",
    "- 동일한 장치에 있어야 연산 가능\n",
    "\n",
    "---\n",
    "\n",
    "###### **Line 11: Forward Pass (예측)**\n",
    "\n",
    "```python\n",
    "preds = model(data)\n",
    "```\n",
    "\n",
    "**실제 동작:**\n",
    "\n",
    "```python\n",
    "# model(data)는 model.forward(data)를 호출\n",
    "preds = model.forward(data)\n",
    "\n",
    "# MLP 내부에서:\n",
    "# x = x.view(x.size(0), -1)  # 이미 reshape 됨\n",
    "# x = relu(fc1(x))\n",
    "# x = relu(fc2(x))\n",
    "# x = fc3(x)\n",
    "# return x\n",
    "```\n",
    "\n",
    "**출력 형태:**\n",
    "\n",
    "```python\n",
    "data.shape:   (32, 784)    # 입력\n",
    "preds.shape:  (32, 10)     # 출력\n",
    "\n",
    "# 예시 출력 (1개 샘플)\n",
    "preds[0] = tensor([-2.3, 0.5, 1.2, 3.5, -0.8, 0.2, -1.5, 2.1, -0.3, 0.9])\n",
    "                    ↑    ↑    ↑    ↑     ↑    ↑     ↑    ↑     ↑    ↑\n",
    "                   class 0~9에 대한 점수 (logits)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###### **Line 13: Loss 계산**\n",
    "\n",
    "```python\n",
    "loss = nn.CrossEntropyLoss()(preds, label.to(device))\n",
    "```\n",
    "\n",
    "**분해해서 이해하기:**\n",
    "\n",
    "```python\n",
    "# 방법 1 (강사님 방식 - 한 줄)\n",
    "loss = nn.CrossEntropyLoss()(preds, label.to(device))\n",
    "\n",
    "# 방법 2 (분해)\n",
    "criterion = nn.CrossEntropyLoss()        # Loss 함수 생성\n",
    "loss = criterion(preds, label.to(device)) # Loss 계산\n",
    "```\n",
    "\n",
    "**CrossEntropyLoss의 역할:**\n",
    "\n",
    "```python\n",
    "# 예시\n",
    "preds[0] = [-2.3, 0.5, 1.2, 3.5, ...]  # 모델 예측\n",
    "label[0] = 3                            # 정답 클래스\n",
    "\n",
    "# 내부 동작:\n",
    "# 1. Softmax로 확률 변환\n",
    "probs = softmax(preds[0]) \n",
    "# → [0.001, 0.020, 0.038, 0.900, ...]\n",
    "\n",
    "# 2. 정답 클래스의 확률 추출\n",
    "target_prob = probs[3] = 0.900\n",
    "\n",
    "# 3. Loss 계산 (음의 로그 확률)\n",
    "loss = -log(0.900) = 0.105\n",
    "\n",
    "# 확률이 높을수록 (정확할수록) loss가 낮음!\n",
    "```\n",
    "\n",
    "**Loss 값의 의미:**\n",
    "\n",
    "| Loss | 의미 | 상태 |\n",
    "|------|------|------|\n",
    "| **0.0** | 완벽한 예측 | 이상적 (실제로는 불가능) |\n",
    "| **0.1 ~ 0.5** | 매우 좋은 예측 | 잘 학습됨 |\n",
    "| **1.0 ~ 2.0** | 괜찮은 예측 | 학습 중 |\n",
    "| **2.3** | 랜덤 수준 (10개 클래스) | 초기 상태 |\n",
    "| **10+** | 매우 나쁜 예측 | 문제 있음 |\n",
    "\n",
    "**`label.to(device)`의 이유:**\n",
    "```python\n",
    "preds: GPU에 있음\n",
    "label: CPU에 있음\n",
    "→ 오류 발생!\n",
    "\n",
    "label.to(device)로 GPU로 이동 필요\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###### **Line 14: Backward Pass (기울기 계산)**\n",
    "\n",
    "```python\n",
    "loss.backward()\n",
    "```\n",
    "\n",
    "**마법의 한 줄: 자동 미분 (Autograd)**\n",
    "\n",
    "```python\n",
    "# 이 한 줄이 하는 일:\n",
    "# 1. Loss에서 모든 파라미터까지 역전파\n",
    "# 2. 각 파라미터의 gradient 계산\n",
    "# 3. param.grad에 저장\n",
    "```\n",
    "\n",
    "**내부 동작 시각화:**\n",
    "\n",
    "```\n",
    "Forward Pass:\n",
    "Input → FC1 → ReLU → FC2 → ReLU → FC3 → Loss\n",
    "                                          ↓\n",
    "\n",
    "Backward Pass (loss.backward()):\n",
    "←───────←──────←──────←──────←──────← Loss\n",
    "gradient 계산하며 역방향 전파\n",
    "```\n",
    "\n",
    "**결과:**\n",
    "\n",
    "```python\n",
    "# backward() 실행 전\n",
    "model.fc1.weight.grad  # None\n",
    "\n",
    "# backward() 실행 후\n",
    "model.fc1.weight.grad  # tensor([[0.001, -0.002, ...]])\n",
    "model.fc1.bias.grad    # tensor([0.003, ...])\n",
    "model.fc2.weight.grad  # tensor([[...]])\n",
    "# ... 모든 파라미터의 gradient 계산됨\n",
    "```\n",
    "\n",
    "**Gradient의 의미:**\n",
    "```\n",
    "gradient > 0: 가중치 증가 시 Loss 증가 → 가중치 감소 필요\n",
    "gradient < 0: 가중치 증가 시 Loss 감소 → 가중치 증가 필요\n",
    "gradient = 0: 최적점 (이론적)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###### **Line 15: 가중치 업데이트**\n",
    "\n",
    "```python\n",
    "optim.step()\n",
    "```\n",
    "\n",
    "**실제 학습이 일어나는 순간!**\n",
    "\n",
    "```python\n",
    "# optim.step()이 하는 일:\n",
    "\n",
    "for param in model.parameters():\n",
    "    # Adam 알고리즘 적용\n",
    "    # param = param - lr × gradient (단순화)\n",
    "    \n",
    "    # 실제는 더 복잡:\n",
    "    # - 모멘텀 계산\n",
    "    # - 적응형 학습률 계산\n",
    "    # - 가중치 업데이트\n",
    "    param.data = param.data - lr × adjusted_gradient\n",
    "```\n",
    "\n",
    "**예시 (단순화):**\n",
    "\n",
    "```python\n",
    "# 업데이트 전\n",
    "fc1.weight[0][0] = 0.532\n",
    "\n",
    "# gradient\n",
    "fc1.weight.grad[0][0] = 0.012\n",
    "\n",
    "# 업데이트 (lr=0.001)\n",
    "fc1.weight[0][0] = 0.532 - 0.001 × 0.012\n",
    "                 = 0.532 - 0.000012\n",
    "                 = 0.531988  ✓\n",
    "```\n",
    "\n",
    "**중요:** \n",
    "- `optim.step()` 없으면 학습 안 됨!\n",
    "- Gradient만 계산하고 업데이트하지 않는 것\n",
    "\n",
    "---\n",
    "\n",
    "###### **Line 17: Loss 출력**\n",
    "\n",
    "```python\n",
    "print(f\"Epoch {epoch+1} | loss: {loss.item():.4f}\")\n",
    "```\n",
    "\n",
    "**각 요소 분석:**\n",
    "\n",
    "```python\n",
    "f\"Epoch {epoch+1} | loss: {loss.item():.4f}\"\n",
    "#   ↑      ↑               ↑        ↑\n",
    "#   f-string epoch 번호    tensor→float  소수점 4자리\n",
    "```\n",
    "\n",
    "####### **`epoch+1`의 이유:**\n",
    "\n",
    "```python\n",
    "range(20)  # 0, 1, 2, ..., 19\n",
    "epoch+1    # 1, 2, 3, ..., 20 (사람이 읽기 편함)\n",
    "```\n",
    "\n",
    "####### **`loss.item()`의 의미:**\n",
    "\n",
    "```python\n",
    "loss         # tensor(0.5234, grad_fn=<NllLossBackward>)\n",
    "loss.item()  # 0.5234 (Python float)\n",
    "\n",
    "# 왜 필요한가?\n",
    "print(loss)       # tensor(0.5234, grad_fn=...) - 복잡함\n",
    "print(loss.item()) # 0.5234 - 깔끔함\n",
    "```\n",
    "\n",
    "####### **`.4f`의 의미:**\n",
    "\n",
    "```python\n",
    ":.4f  # 소수점 4자리까지 표시\n",
    "\n",
    "loss.item() = 0.52341287\n",
    "f\"{loss.item():.4f}\"  # \"0.5234\"\n",
    "f\"{loss.item():.2f}\"  # \"0.52\"\n",
    "```\n",
    "\n",
    "**출력 예시:**\n",
    "\n",
    "```\n",
    "Epoch 1 | loss: 2.3045\n",
    "Epoch 2 | loss: 0.8912\n",
    "Epoch 3 | loss: 0.4523\n",
    "...\n",
    "Epoch 20 | loss: 0.1234\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔄 전체 학습 과정 흐름도\n",
    "\n",
    "##### 한 Epoch의 전체 흐름\n",
    "\n",
    "```python\n",
    "for epoch in range(20):                              # Epoch 반복\n",
    "    for data, label in train_loader:                 # Batch 반복 (1875회)\n",
    "        # ────────────────────────────────────────\n",
    "        # 1. 초기화\n",
    "        optim.zero_grad()                            # Gradient 초기화\n",
    "        \n",
    "        # 2. Forward Pass\n",
    "        data = torch.reshape(data, (-1, 784)).to(device)  # 전처리\n",
    "        preds = model(data)                          # 예측\n",
    "        \n",
    "        # 3. Loss 계산\n",
    "        loss = nn.CrossEntropyLoss()(preds, label.to(device))\n",
    "        \n",
    "        # 4. Backward Pass\n",
    "        loss.backward()                              # Gradient 계산\n",
    "        \n",
    "        # 5. 업데이트\n",
    "        optim.step()                                 # 가중치 업데이트\n",
    "        # ────────────────────────────────────────\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} | loss: {loss.item():.4f}\")\n",
    "```\n",
    "\n",
    "##### 시간 순서대로 상세 분석\n",
    "\n",
    "```\n",
    "Epoch 1 시작\n",
    "│\n",
    "├─ Batch 1 (32개 이미지)\n",
    "│  ├─ zero_grad():     gradient = 0으로 초기화\n",
    "│  ├─ forward():       예측값 계산\n",
    "│  ├─ loss 계산:       예측과 정답 비교 → 2.3045\n",
    "│  ├─ backward():      gradient 계산\n",
    "│  └─ step():          가중치 업데이트\n",
    "│\n",
    "├─ Batch 2 (32개 이미지)\n",
    "│  ├─ zero_grad():     gradient = 0으로 초기화\n",
    "│  ├─ forward():       예측값 계산\n",
    "│  ├─ loss 계산:       예측과 정답 비교 → 2.2891\n",
    "│  ├─ backward():      gradient 계산\n",
    "│  └─ step():          가중치 업데이트\n",
    "│\n",
    "├─ ...\n",
    "│\n",
    "└─ Batch 1875 (마지막)\n",
    "   └─ loss: 0.5234\n",
    "\n",
    "Epoch 1 완료 → \"Epoch 1 | loss: 0.5234\" 출력\n",
    "\n",
    "Epoch 2 시작\n",
    "│\n",
    "└─ ... (반복)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 💡 핵심 개념 정리\n",
    "\n",
    "##### ✅ 5단계 학습 사이클\n",
    "\n",
    "```\n",
    "1. zero_grad()    → 과거 gradient 지우기\n",
    "2. forward()      → 예측하기\n",
    "3. loss 계산      → 얼마나 틀렸는지 측정\n",
    "4. backward()     → 어떻게 고칠지 계산 (gradient)\n",
    "5. step()         → 실제로 고치기 (가중치 업데이트)\n",
    "\n",
    "→ 이 과정을 수천~수만 번 반복!\n",
    "```\n",
    "\n",
    "##### ✅ 용어 정리\n",
    "\n",
    "| 용어 | 의미 | 비유 |\n",
    "|------|------|------|\n",
    "| **Optimizer** | 학습 알고리즘 | 내비게이션 |\n",
    "| **Learning Rate** | 학습 속도 | 자동차 속도 |\n",
    "| **Gradient** | 개선 방향 | 내비게이션 화살표 |\n",
    "| **Loss** | 오차 | 목적지까지 거리 |\n",
    "| **Epoch** | 전체 데이터 1회 | 교재 1회독 |\n",
    "| **Batch** | 데이터 묶음 | 교재 1챕터 |\n",
    "\n",
    "##### ✅ 왜 이런 순서인가?\n",
    "\n",
    "```python\n",
    "# 잘못된 순서 ❌\n",
    "optim.step()       # 뭘 업데이트? gradient 없음!\n",
    "loss.backward()    # gradient 계산\n",
    "optim.zero_grad()  # 방금 계산한 gradient 지움!\n",
    "\n",
    "# 올바른 순서 ✓\n",
    "optim.zero_grad()  # 1. 과거 정리\n",
    "# forward & loss   # 2. 예측 & 평가\n",
    "loss.backward()    # 3. 개선 방향 계산\n",
    "optim.step()       # 4. 실제 개선\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🎯 자주 하는 실수와 해결\n",
    "\n",
    "##### 실수 1: zero_grad() 빠뜨림\n",
    "\n",
    "```python\n",
    "# 잘못된 코드 ❌\n",
    "for data, label in train_loader:\n",
    "    # optim.zero_grad()  ← 빠뜨림!\n",
    "    preds = model(data)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "# 결과: gradient가 누적됨 → 잘못된 학습\n",
    "```\n",
    "\n",
    "##### 실수 2: device 불일치\n",
    "\n",
    "```python\n",
    "# 잘못된 코드 ❌\n",
    "model.to('cuda')\n",
    "data = data  # CPU에 그대로\n",
    "preds = model(data)  # 오류!\n",
    "\n",
    "# 올바른 코드 ✓\n",
    "model.to(device)\n",
    "data = data.to(device)\n",
    "preds = model(data)\n",
    "```\n",
    "\n",
    "### 실수 3: step() 빠뜨림\n",
    "\n",
    "```python\n",
    "# 잘못된 코드 ❌\n",
    "optim.zero_grad()\n",
    "loss.backward()\n",
    "# optim.step()  ← 빠뜨림!\n",
    "\n",
    "# 결과: gradient만 계산, 업데이트 안 됨 → 학습 안 됨\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🚀 실전 팁\n",
    "\n",
    "##### Adam의 대안들\n",
    "\n",
    "```python\n",
    "# SGD (간단하지만 느림)\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Adam (범용적, 추천) ⭐\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# AdamW (최신, 성능 개선)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "\n",
    "# RMSprop (RNN에 좋음)\n",
    "optim = torch.optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "```\n",
    "\n",
    "##### Learning Rate 스케줄링\n",
    "\n",
    "```python\n",
    "# 학습 중간에 learning rate 감소\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "optim = Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = StepLR(optim, step_size=10, gamma=0.1)\n",
    "\n",
    "for epoch in range(20):\n",
    "    # 학습...\n",
    "    scheduler.step()  # 10 epoch마다 lr × 0.1\n",
    "```\n",
    "\n",
    "##### 학습 모니터링 개선\n",
    "\n",
    "```python\n",
    "# 더 상세한 출력\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    for data, label in train_loader:\n",
    "        # ... 학습 코드 ...\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} | Avg Loss: {avg_loss:.4f}\")\n",
    "```\n",
    "\n",
    "이제 강사님의 학습 코드를 완벽하게 이해하셨습니다! 🎉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac62b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 가중치 저장, model.state_dict() 형태로 저장 → 모델의 가중치만 저장\n",
    "torch.save(model.state_dict(), \"MNIST.pt\") # pt, pth, pkl, json, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a9cb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 로드 -> 우리는 모델을 저장한 것은 아니므로, 모델을 생성해야 함\n",
    "model.load_state_dict(torch.load(\"MNIST.pt\", map_location=device)) # map_location=device → 모델을 저장한 장치와 동일한 장치에 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1867a8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9791\n"
     ]
    }
   ],
   "source": [
    "# 모델 에측 및 평가\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in test_loader:\n",
    "        data = torch.reshape(data, (-1, 784)).to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        preds = output.data.max(1)[1]\n",
    "\n",
    "        corr = preds.eq(label.to(device).data).sum().item()\n",
    "        correct += corr\n",
    "    \n",
    "    print(f\"Accuracy: {correct / len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f90511a",
   "metadata": {},
   "source": [
    "### 모델 평가(Evaluation) 코드 완전 분석\n",
    "\n",
    "#### 🧠 Chain of Thought: 평가 vs 학습의 차이\n",
    "\n",
    "##### 근본적인 질문: 왜 평가 코드가 학습 코드와 다른가?\n",
    "\n",
    "```\n",
    "학습(Training):\n",
    "- 목적: 가중치 업데이트\n",
    "- Gradient 필요: ✓ (backward 필요)\n",
    "- 데이터: train_loader\n",
    "- 모드: 학습 모드\n",
    "\n",
    "평가(Evaluation):\n",
    "- 목적: 성능 측정\n",
    "- Gradient 필요: ✗ (backward 불필요)\n",
    "- 데이터: test_loader\n",
    "- 모드: 평가 모드\n",
    "```\n",
    "\n",
    "**핵심 차이점:**\n",
    "- 평가할 때는 **가중치를 업데이트하지 않음**\n",
    "- 따라서 **gradient 계산이 불필요** → 메모리 절약, 속도 향상\n",
    "- **새로운 데이터(test set)**로 성능 측정\n",
    "\n",
    "---\n",
    "\n",
    "#### 📝 코드 전체 구조 이해\n",
    "\n",
    "##### 학습 vs 평가 코드 비교\n",
    "\n",
    "```python\n",
    "# ========== 학습 코드 ==========\n",
    "for epoch in range(20):\n",
    "    for data, label in train_loader:    # 학습 데이터\n",
    "        optim.zero_grad()               # Gradient 초기화 ✓\n",
    "        preds = model(data)             # 예측\n",
    "        loss = criterion(preds, label)  # Loss 계산 ✓\n",
    "        loss.backward()                 # Gradient 계산 ✓\n",
    "        optim.step()                    # 가중치 업데이트 ✓\n",
    "\n",
    "# ========== 평가 코드 ==========\n",
    "correct = 0\n",
    "with torch.no_grad():                   # Gradient 계산 비활성화 ✓\n",
    "    for data, label in test_loader:     # 테스트 데이터\n",
    "        output = model(data)            # 예측\n",
    "        # loss 계산 없음                # Loss 불필요 ✗\n",
    "        # backward 없음                 # Gradient 계산 불필요 ✗\n",
    "        # step 없음                     # 가중치 업데이트 불필요 ✗\n",
    "        \n",
    "        # 정확도 계산만 수행\n",
    "        preds = output.data.max(1)[1]\n",
    "        correct += preds.eq(label).sum().item()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔍 코드 라인별 상세 분석\n",
    "\n",
    "##### Line 2: correct 변수 초기화\n",
    "\n",
    "```python\n",
    "correct = 0\n",
    "```\n",
    "\n",
    "###### **왜 0으로 초기화하는가?**\n",
    "\n",
    "**CoT (사고 과정):**\n",
    "\n",
    "```\n",
    "테스트 데이터: 10,000개 이미지\n",
    "목표: 몇 개를 맞췄는지 세기\n",
    "\n",
    "방법:\n",
    "1. correct = 0으로 시작\n",
    "2. 각 배치마다 맞춘 개수 더하기\n",
    "3. 최종: 전체 맞춘 개수 / 전체 개수 = 정확도\n",
    "```\n",
    "\n",
    "**동작 예시:**\n",
    "\n",
    "```python\n",
    "correct = 0  # 초기값\n",
    "\n",
    "# Batch 1: 32개 중 28개 맞춤\n",
    "correct += 28  # correct = 28\n",
    "\n",
    "# Batch 2: 32개 중 30개 맞춤\n",
    "correct += 30  # correct = 58\n",
    "\n",
    "# Batch 3: 32개 중 29개 맞춤\n",
    "correct += 29  # correct = 87\n",
    "\n",
    "# ...\n",
    "# 모든 배치 완료\n",
    "# correct = 9,523\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = 9523 / 10000 = 0.9523 = 95.23%\n",
    "```\n",
    "\n",
    "**역할:**\n",
    "- **누적 카운터(Accumulator)** 역할\n",
    "- 전체 테스트 데이터에서 맞춘 개수를 누적\n",
    "- 초기화하지 않으면 이전 값이 남아있음 → 잘못된 계산\n",
    "\n",
    "---\n",
    "\n",
    "##### Line 4: with torch.no_grad()\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "```\n",
    "\n",
    "###### **`with` 문이란?**\n",
    "\n",
    "**Python의 Context Manager (컨텍스트 관리자)**\n",
    "\n",
    "```python\n",
    "# with 문의 기본 구조\n",
    "with [expression] as [variable]:\n",
    "    # 코드 블록\n",
    "    pass\n",
    "\n",
    "# with 문이 하는 일:\n",
    "# 1. 진입 시: __enter__() 호출 (설정)\n",
    "# 2. 블록 실행\n",
    "# 3. 종료 시: __exit__() 호출 (정리)\n",
    "```\n",
    "\n",
    "**일상적인 예시:**\n",
    "\n",
    "```python\n",
    "# 파일 처리 (가장 흔한 예)\n",
    "with open('file.txt', 'r') as f:\n",
    "    content = f.read()\n",
    "    # 자동으로 파일 닫힘 (f.close() 불필요)\n",
    "\n",
    "# with 없이 작성하면:\n",
    "f = open('file.txt', 'r')\n",
    "try:\n",
    "    content = f.read()\n",
    "finally:\n",
    "    f.close()  # 수동으로 닫아야 함\n",
    "```\n",
    "\n",
    "**장점:**\n",
    "- 자동으로 **설정(setup)과 정리(cleanup)** 처리\n",
    "- 코드가 간결해짐\n",
    "- 에러 발생해도 안전하게 정리됨\n",
    "\n",
    "---\n",
    "\n",
    "###### **`torch.no_grad()`란?**\n",
    "\n",
    "**Gradient 계산을 비활성화하는 Context Manager**\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    # 이 블록 안에서는 gradient 계산 안 함\n",
    "    output = model(data)\n",
    "    # gradient 추적 비활성화 상태\n",
    "```\n",
    "\n",
    "###### **왜 필요한가? (CoT)**\n",
    "\n",
    "**시나리오 1: 평가 시 no_grad() 사용 안 하면**\n",
    "\n",
    "```python\n",
    "# torch.no_grad() 없이\n",
    "for data, label in test_loader:\n",
    "    output = model(data)  \n",
    "    # PyTorch는 자동으로:\n",
    "    # 1. 모든 연산 기록 (computation graph 생성)\n",
    "    # 2. Gradient 계산 준비\n",
    "    # 3. 중간 값들 메모리에 저장\n",
    "    \n",
    "    # 문제:\n",
    "    # - backward()를 호출하지 않을 건데 불필요한 작업\n",
    "    # - 메모리 낭비\n",
    "    # - 속도 느림\n",
    "```\n",
    "\n",
    "**메모리 사용량 비교:**\n",
    "\n",
    "```python\n",
    "# no_grad() 없이\n",
    "import torch\n",
    "x = torch.randn(1000, 1000)\n",
    "model = LargeModel()\n",
    "output = model(x)\n",
    "# 메모리 사용: ~500MB (gradient 정보 저장)\n",
    "\n",
    "# no_grad() 사용\n",
    "with torch.no_grad():\n",
    "    output = model(x)\n",
    "# 메모리 사용: ~100MB (gradient 정보 저장 안 함)\n",
    "\n",
    "# 약 5배 메모리 절약!\n",
    "```\n",
    "\n",
    "**시나리오 2: no_grad() 사용**\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    output = model(data)\n",
    "    # PyTorch는:\n",
    "    # 1. 연산만 수행\n",
    "    # 2. Gradient 계산 준비 안 함\n",
    "    # 3. 중간 값 저장 안 함\n",
    "    \n",
    "    # 장점:\n",
    "    # ✓ 메모리 절약\n",
    "    # ✓ 속도 향상\n",
    "    # ✓ 명확한 의도 표현\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###### **내부 동작 원리**\n",
    "\n",
    "```python\n",
    "# 정상적인 forward pass (학습 시)\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = x * 2\n",
    "z = y + 3\n",
    "print(z.requires_grad)  # True (gradient 추적 중)\n",
    "\n",
    "# no_grad() 사용 (평가 시)\n",
    "with torch.no_grad():\n",
    "    x = torch.tensor([1.0], requires_grad=True)\n",
    "    y = x * 2\n",
    "    z = y + 3\n",
    "    print(z.requires_grad)  # False (gradient 추적 안 함)\n",
    "```\n",
    "\n",
    "**시각화:**\n",
    "\n",
    "```\n",
    "학습 시 (Gradient 추적):\n",
    "Input → Conv1 → ReLU → ... → Output\n",
    "  ↓       ↓       ↓            ↓\n",
    "[저장]  [저장]  [저장]      [저장]\n",
    "모든 중간 값 메모리에 보관 (backward를 위해)\n",
    "\n",
    "평가 시 (no_grad):\n",
    "Input → Conv1 → ReLU → ... → Output\n",
    "  ↓       ↓       ↓            ↓\n",
    "[무시]  [무시]  [무시]      [무시]\n",
    "중간 값 저장 안 함 → 메모리 절약\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###### **with와 torch.no_grad()의 결합**\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    # 진입 시: gradient 추적 비활성화\n",
    "    for data, label in test_loader:\n",
    "        output = model(data)\n",
    "        # 모든 연산이 gradient 추적 없이 수행\n",
    "    # 종료 시: 원래 상태로 복구\n",
    "\n",
    "# with 블록 밖에서는 다시 gradient 추적됨\n",
    "```\n",
    "\n",
    "**대안 (권장하지 않음):**\n",
    "\n",
    "```python\n",
    "# torch.no_grad() 데코레이터 방식\n",
    "@torch.no_grad()\n",
    "def evaluate(model, test_loader):\n",
    "    # 함수 전체에서 gradient 비활성화\n",
    "    pass\n",
    "\n",
    "# 수동 설정 (번거로움)\n",
    "torch.set_grad_enabled(False)\n",
    "# 평가 코드\n",
    "torch.set_grad_enabled(True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### Line 5-6: 데이터 로드 및 전처리\n",
    "\n",
    "```python\n",
    "for data, label in test_loader:\n",
    "    data = torch.reshape(data, (-1, 784)).to(device)\n",
    "```\n",
    "\n",
    "###### **왜 또 reshape을 해야 하는가?**\n",
    "\n",
    "**CoT (사고 과정):**\n",
    "\n",
    "```\n",
    "질문: 학습할 때도 reshape 했는데 왜 또 해야 하나?\n",
    "\n",
    "답변: test_loader도 원본 데이터를 반환하기 때문\n",
    "```\n",
    "\n",
    "**test_loader의 데이터 형태:**\n",
    "\n",
    "```python\n",
    "# test_loader에서 가져온 데이터\n",
    "data.shape: (32, 1, 28, 28)\n",
    "            ↑   ↑   ↑   ↑\n",
    "         batch ch  H   W\n",
    "\n",
    "# MLP 모델이 원하는 형태\n",
    "required: (32, 784)\n",
    "          ↑    ↑\n",
    "       batch flattened\n",
    "```\n",
    "\n",
    "**왜 DataLoader가 자동으로 reshape 안 해주나?**\n",
    "\n",
    "```python\n",
    "# DataLoader는 원본 데이터만 제공\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# 장점:\n",
    "# 1. 범용성: CNN과 MLP 모두 사용 가능\n",
    "#    - CNN: (B, 1, 28, 28) 그대로 사용\n",
    "#    - MLP: (B, 1, 28, 28) → (B, 784)로 변환\n",
    "# 2. 유연성: 모델에 따라 전처리 다르게 가능\n",
    "```\n",
    "\n",
    "**학습 vs 평가 코드 비교:**\n",
    "\n",
    "```python\n",
    "# 학습 코드\n",
    "for data, label in train_loader:\n",
    "    data = torch.reshape(data, (-1, 784)).to(device)  # reshape 필요\n",
    "    # ...\n",
    "\n",
    "# 평가 코드\n",
    "for data, label in test_loader:\n",
    "    data = torch.reshape(data, (-1, 784)).to(device)  # 똑같이 reshape 필요\n",
    "    # ...\n",
    "\n",
    "# 이유: train_loader와 test_loader 모두 동일한 원본 형태 제공\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### Line 8: Forward Pass (예측)\n",
    "\n",
    "```python\n",
    "output = model(data)\n",
    "```\n",
    "\n",
    "**학습 시와 동일한 과정:**\n",
    "\n",
    "```python\n",
    "# 입력\n",
    "data.shape: (32, 784)\n",
    "\n",
    "# MLP forward 내부\n",
    "x = x.view(x.size(0), -1)  # 이미 (32, 784)라서 변화 없음\n",
    "x = relu(fc1(x))           # (32, 784) → (32, 128)\n",
    "x = relu(fc2(x))           # (32, 128) → (32, 64)\n",
    "x = fc3(x)                 # (32, 64) → (32, 10)\n",
    "\n",
    "# 출력\n",
    "output.shape: (32, 10)\n",
    "```\n",
    "\n",
    "**output의 내용 (예시):**\n",
    "\n",
    "```python\n",
    "output[0] = tensor([-2.3, 0.5, 1.2, 3.5, -0.8, 0.2, -1.5, 2.1, -0.3, 0.9])\n",
    "                    ↑    ↑    ↑    ↑     ↑    ↑     ↑    ↑     ↑    ↑\n",
    "                   class 0~9에 대한 점수 (logits)\n",
    "\n",
    "# 가장 큰 값: 3.5 (index 3)\n",
    "# 예측: 이 이미지는 숫자 \"3\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### Line 9: 예측 클래스 추출\n",
    "\n",
    "```python\n",
    "preds = output.data.max(1)[1]\n",
    "```\n",
    "\n",
    "###### **단계별 분해 분석**\n",
    "\n",
    "이 한 줄은 **3가지 연산**이 결합된 복잡한 코드입니다!\n",
    "\n",
    "####### **Step 1: `output.data`**\n",
    "\n",
    "```python\n",
    "output.data\n",
    "```\n",
    "\n",
    "**의미:**\n",
    "- `output`: Tensor (gradient 정보 포함 가능)\n",
    "- `output.data`: Tensor의 실제 데이터만 (gradient 정보 제거)\n",
    "\n",
    "**차이점:**\n",
    "\n",
    "```python\n",
    "# 학습 중\n",
    "output = model(data)\n",
    "print(output.requires_grad)      # True (gradient 추적)\n",
    "print(output.data.requires_grad) # False (데이터만)\n",
    "\n",
    "# 평가 중 (no_grad 안에서)\n",
    "with torch.no_grad():\n",
    "    output = model(data)\n",
    "    print(output.requires_grad)  # False (이미 추적 안 함)\n",
    "    \n",
    "    # 사실 no_grad() 안에서는 .data 불필요\n",
    "    # output == output.data (동일)\n",
    "```\n",
    "\n",
    "**참고:** 현대적인 코드에서는 `.data` 사용을 권장하지 않습니다.\n",
    "```python\n",
    "# 구식 (강사님 코드)\n",
    "preds = output.data.max(1)[1]\n",
    "\n",
    "# 현대적\n",
    "preds = output.max(1)[1]  # .data 없어도 됨 (no_grad 안에서)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "####### **Step 2: `.max(1)`**\n",
    "\n",
    "```python\n",
    "output.max(1)\n",
    "```\n",
    "\n",
    "**매우 중요한 부분!**\n",
    "\n",
    "**`.max(dim)` 메서드:**\n",
    "- 지정한 차원(dimension)에서 최댓값 찾기\n",
    "- **반환값: (values, indices) 튜플**\n",
    "\n",
    "**dim=1의 의미:**\n",
    "\n",
    "```python\n",
    "output.shape: (32, 10)\n",
    "              ↑    ↑\n",
    "            dim=0 dim=1\n",
    "\n",
    "# dim=0: 배치 방향 (세로)\n",
    "# dim=1: 클래스 방향 (가로) ← 이것을 사용!\n",
    "```\n",
    "\n",
    "**시각화:**\n",
    "\n",
    "```python\n",
    "# output 예시 (batch_size=3으로 단순화)\n",
    "output = tensor([\n",
    "    [-2.3, 0.5, 1.2, 3.5, -0.8, 0.2, -1.5, 2.1, -0.3, 0.9],  # 샘플 0\n",
    "    [1.2, -0.3, 5.6, 0.8, 1.1, -2.1, 0.5, 0.3, 0.9, -0.5],   # 샘플 1\n",
    "    [0.3, 0.7, 0.2, 0.5, 0.9, 3.2, -1.1, 0.4, -0.2, 1.5]     # 샘플 2\n",
    "])\n",
    "#     0    1    2    3    4    5     6    7     8     9   ← 클래스 인덱스\n",
    "\n",
    "# max(1) 적용: 각 행(샘플)에서 최댓값 찾기\n",
    "values, indices = output.max(1)\n",
    "\n",
    "values = tensor([3.5, 5.6, 3.2])    # 각 샘플의 최댓값\n",
    "indices = tensor([3, 2, 5])          # 최댓값의 위치 (클래스)\n",
    "#                 ↑  ↑  ↑\n",
    "#              class 3, 2, 5 예측\n",
    "```\n",
    "\n",
    "**dim에 따른 차이:**\n",
    "\n",
    "```python\n",
    "# dim=0 (열 방향, 배치 간 비교) - 거의 사용 안 함\n",
    "output.max(0)  \n",
    "# → 모든 샘플 중 각 클래스의 최댓값 (의미 없음)\n",
    "\n",
    "# dim=1 (행 방향, 클래스 간 비교) ⭐\n",
    "output.max(1)\n",
    "# → 각 샘플의 예측 클래스 (우리가 원하는 것!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "####### **Step 3: `[1]` 인덱싱**\n",
    "\n",
    "```python\n",
    "output.max(1)[1]\n",
    "```\n",
    "\n",
    "**튜플 인덱싱:**\n",
    "\n",
    "```python\n",
    "values, indices = output.max(1)\n",
    "# values:  tensor([3.5, 5.6, 3.2])  ← [0]\n",
    "# indices: tensor([3, 2, 5])        ← [1]\n",
    "\n",
    "preds = output.max(1)[1]\n",
    "# = indices\n",
    "# = tensor([3, 2, 5])  # 예측 클래스\n",
    "```\n",
    "\n",
    "**왜 [1]만 필요한가?**\n",
    "\n",
    "```\n",
    "values (최댓값):   정확도 계산에 불필요\n",
    "                  → 어떤 클래스인지만 중요\n",
    "\n",
    "indices (위치):    예측한 클래스 번호\n",
    "                  → 정답과 비교하기 위해 필요! ✓\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###### **전체 과정 종합**\n",
    "\n",
    "```python\n",
    "# 원본 코드\n",
    "preds = output.data.max(1)[1]\n",
    "\n",
    "# 단계별 분해\n",
    "step1 = output.data              # gradient 정보 제거 (실제로는 불필요)\n",
    "step2 = step1.max(1)             # (values, indices) 튜플\n",
    "step3 = step2[1]                 # indices만 추출\n",
    "preds = step3                    # 최종 예측 클래스\n",
    "\n",
    "# 예시\n",
    "output = tensor([[−2.3, 0.5, ..., 3.5, ...],  # 샘플 0: 3.5가 최대\n",
    "                 [1.2, −0.3, ..., 5.6, ...],  # 샘플 1: 5.6이 최대\n",
    "                 ...])\n",
    "\n",
    "preds = tensor([3, 2, 5, 1, 0, ...])  # 각 샘플의 예측 클래스\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### Line 11: 정답 개수 계산\n",
    "\n",
    "```python\n",
    "corr = preds.eq(label.to(device).data).sum().item()\n",
    "```\n",
    "\n",
    "###### **복잡한 한 줄을 분해하기**\n",
    "\n",
    "이 코드는 **5가지 연산**이 연결되어 있습니다!\n",
    "\n",
    "####### **Step 1: `label.to(device)`**\n",
    "\n",
    "```python\n",
    "label.to(device)\n",
    "```\n",
    "\n",
    "**이유:**\n",
    "- `preds`: GPU에 있음 (model이 GPU에 있었으므로)\n",
    "- `label`: CPU에 있음 (test_loader에서 가져온 원본)\n",
    "- 비교하려면 같은 장치에 있어야 함\n",
    "\n",
    "```python\n",
    "# 오류 발생 ❌\n",
    "preds (GPU) == label (CPU)  \n",
    "# RuntimeError: Expected all tensors to be on the same device\n",
    "\n",
    "# 올바른 방법 ✓\n",
    "preds (GPU) == label.to(device) (GPU)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "####### **Step 2: `.data` (선택사항)**\n",
    "\n",
    "```python\n",
    "label.to(device).data\n",
    "```\n",
    "\n",
    "**의미:** \n",
    "- label의 실제 값만 추출\n",
    "- 현대적인 코드에서는 생략 가능\n",
    "\n",
    "---\n",
    "\n",
    "####### **Step 3: `.eq(...)` (Equal)**\n",
    "\n",
    "```python\n",
    "preds.eq(label.to(device).data)\n",
    "```\n",
    "\n",
    "**`eq()` = Element-wise Equality (원소별 비교)**\n",
    "\n",
    "```python\n",
    "# 예시\n",
    "preds = tensor([3, 2, 5, 1, 0, 7, 9, 4])  # 예측값\n",
    "label = tensor([3, 7, 5, 1, 0, 7, 8, 4])  # 정답\n",
    "\n",
    "result = preds.eq(label)\n",
    "# tensor([True, False, True, True, True, True, False, True])\n",
    "#          ↑     ↑      ↑     ↑     ↑     ↑      ↑      ↑\n",
    "#         맞음  틀림   맞음  맞음  맞음  맞음   틀림   맞음\n",
    "```\n",
    "\n",
    "**대안 표현:**\n",
    "\n",
    "```python\n",
    "# 방법 1 (강사님 코드)\n",
    "preds.eq(label)\n",
    "\n",
    "# 방법 2 (더 직관적)\n",
    "preds == label\n",
    "\n",
    "# 방법 3 (명시적)\n",
    "torch.eq(preds, label)\n",
    "\n",
    "# 모두 동일한 결과!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "####### **Step 4: `.sum()`**\n",
    "\n",
    "```python\n",
    "preds.eq(label.to(device).data).sum()\n",
    "```\n",
    "\n",
    "**True를 1로, False를 0으로 세기**\n",
    "\n",
    "```python\n",
    "result = tensor([True, False, True, True, True, True, False, True])\n",
    "\n",
    "# sum() 적용\n",
    "count = result.sum()\n",
    "# tensor(6)\n",
    "\n",
    "# 내부 동작:\n",
    "# True → 1\n",
    "# False → 0\n",
    "# 1 + 0 + 1 + 1 + 1 + 1 + 0 + 1 = 6\n",
    "```\n",
    "\n",
    "**의미:**\n",
    "- 이 배치에서 맞춘 개수 = 6개\n",
    "- 전체 8개 중 6개 정답\n",
    "\n",
    "---\n",
    "\n",
    "####### **Step 5: `.item()`**\n",
    "\n",
    "```python\n",
    "preds.eq(label.to(device).data).sum().item()\n",
    "```\n",
    "\n",
    "**Tensor → Python 숫자 변환**\n",
    "\n",
    "```python\n",
    "count = result.sum()\n",
    "print(type(count))        # <class 'torch.Tensor'>\n",
    "print(count)              # tensor(6)\n",
    "\n",
    "count_int = count.item()\n",
    "print(type(count_int))    # <class 'int'>\n",
    "print(count_int)          # 6\n",
    "```\n",
    "\n",
    "**왜 필요한가?**\n",
    "\n",
    "```python\n",
    "# Tensor를 Python 변수에 누적하면 비효율적\n",
    "correct += count  # Tensor 연산 (느림, 메모리 누적)\n",
    "\n",
    "# Python int로 변환 후 누적 (효율적)\n",
    "correct += count.item()  # 일반 Python 덧셈 (빠름)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###### **전체 과정 시각화**\n",
    "\n",
    "```python\n",
    "# 원본 코드\n",
    "corr = preds.eq(label.to(device).data).sum().item()\n",
    "\n",
    "# 단계별 시각화 (batch_size=8 예시)\n",
    "preds = tensor([3, 2, 5, 1, 0, 7, 9, 4])  # GPU\n",
    "label = tensor([3, 7, 5, 1, 0, 7, 8, 4])  # CPU\n",
    "\n",
    "# Step 1: label을 GPU로\n",
    "label_gpu = label.to(device)  # GPU로 이동\n",
    "\n",
    "# Step 2: 비교\n",
    "comparison = preds.eq(label_gpu)\n",
    "# tensor([True, False, True, True, True, True, False, True])\n",
    "\n",
    "# Step 3: 맞춘 개수 세기\n",
    "count_tensor = comparison.sum()\n",
    "# tensor(6)\n",
    "\n",
    "# Step 4: Python int 변환\n",
    "corr = count_tensor.item()\n",
    "# 6 (int)\n",
    "\n",
    "# 최종: 이 배치에서 6개 정답\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### Line 12: 누적\n",
    "\n",
    "```python\n",
    "correct += corr\n",
    "```\n",
    "\n",
    "**전체 배치에 걸쳐 누적:**\n",
    "\n",
    "```python\n",
    "# 초기값\n",
    "correct = 0\n",
    "\n",
    "# Batch 1: 32개 중 28개 맞춤\n",
    "correct += 28  # correct = 28\n",
    "\n",
    "# Batch 2: 32개 중 30개 맞춤\n",
    "correct += 30  # correct = 58\n",
    "\n",
    "# Batch 3: 32개 중 29개 맞춤\n",
    "correct += 29  # correct = 87\n",
    "\n",
    "# ...\n",
    "\n",
    "# Batch 312 (마지막, 16개)\n",
    "correct += 15  # correct = 9,523\n",
    "\n",
    "# 최종: 전체 10,000개 중 9,523개 맞춤\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### Line 14: 정확도 출력\n",
    "\n",
    "```python\n",
    "print(f\"Accuracy: {correct / len(test_data)}\")\n",
    "```\n",
    "\n",
    "###### **`len(test_data)`의 의미**\n",
    "\n",
    "```python\n",
    "test_data = MNIST(...)  # 테스트 데이터셋\n",
    "len(test_data)          # 10,000 (전체 테스트 샘플 개수)\n",
    "```\n",
    "\n",
    "**왜 `len(test_loader)`가 아닌가?**\n",
    "\n",
    "```python\n",
    "# test_loader: 배치 개수\n",
    "len(test_loader) = 312  # 10,000 / 32 = 312.5 → 312 + 1\n",
    "\n",
    "# test_data: 전체 샘플 개수\n",
    "len(test_data) = 10,000  # 실제 이미지 개수 ✓\n",
    "```\n",
    "\n",
    "**정확도 계산:**\n",
    "\n",
    "```python\n",
    "accuracy = correct / len(test_data)\n",
    "         = 9523 / 10000\n",
    "         = 0.9523\n",
    "         = 95.23%\n",
    "```\n",
    "\n",
    "**출력 예시:**\n",
    "\n",
    "```\n",
    "Accuracy: 0.9523\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔄 전체 평가 과정 흐름도\n",
    "\n",
    "##### 완전한 실행 흐름\n",
    "\n",
    "```python\n",
    "correct = 0  # 1. 카운터 초기화\n",
    "\n",
    "with torch.no_grad():  # 2. Gradient 비활성화 (메모리 절약)\n",
    "    \n",
    "    # 3. 모든 테스트 배치 순회 (312회 반복)\n",
    "    for data, label in test_loader:  # (32, 1, 28, 28), (32,)\n",
    "        \n",
    "        # 4. 전처리\n",
    "        data = torch.reshape(data, (-1, 784)).to(device)  # (32, 784)\n",
    "        \n",
    "        # 5. 예측\n",
    "        output = model(data)  # (32, 10)\n",
    "        \n",
    "        # 6. 최대 확률 클래스 추출\n",
    "        preds = output.data.max(1)[1]  # (32,) - 예측 클래스\n",
    "        \n",
    "        # 7. 정답과 비교\n",
    "        corr = preds.eq(label.to(device).data).sum().item()  # 맞춘 개수\n",
    "        \n",
    "        # 8. 누적\n",
    "        correct += corr\n",
    "    \n",
    "    # 9. 최종 정확도 계산 및 출력\n",
    "    print(f\"Accuracy: {correct / len(test_data)}\")\n",
    "```\n",
    "\n",
    "##### 단계별 데이터 변환\n",
    "\n",
    "```\n",
    "입력 이미지 (Batch 1)\n",
    "  ↓\n",
    "(32, 1, 28, 28) - 원본 형태\n",
    "  ↓ [reshape]\n",
    "(32, 784) - Flatten\n",
    "  ↓ [model]\n",
    "(32, 10) - Logits (각 클래스 점수)\n",
    "  ↓ [max(1)[1]]\n",
    "(32,) - 예측 클래스 [3, 2, 5, 1, ...]\n",
    "  ↓ [eq(label)]\n",
    "(32,) - 정답 여부 [True, False, True, ...]\n",
    "  ↓ [sum()]\n",
    "tensor(28) - 맞춘 개수\n",
    "  ↓ [item()]\n",
    "28 - Python int\n",
    "  ↓ [누적]\n",
    "correct += 28\n",
    "\n",
    "... (모든 배치 반복)\n",
    "\n",
    "최종 정확도: 9523 / 10000 = 95.23%\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 📊 학습 vs 평가 완전 비교\n",
    "\n",
    "##### 코드 구조 비교\n",
    "\n",
    "| 요소 | 학습 (Training) | 평가 (Evaluation) |\n",
    "|------|----------------|-------------------|\n",
    "| **데이터** | `train_loader` | `test_loader` |\n",
    "| **Gradient** | 필요 ✓ | 불필요 ✗ |\n",
    "| **Context** | 없음 | `with torch.no_grad()` |\n",
    "| **zero_grad()** | 매 배치마다 ✓ | 없음 ✗ |\n",
    "| **Loss 계산** | `loss = criterion(...)` ✓ | 없음 ✗ |\n",
    "| **Backward** | `loss.backward()` ✓ | 없음 ✗ |\n",
    "| **Step** | `optim.step()` ✓ | 없음 ✗ |\n",
    "| **평가 지표** | Loss 값 | Accuracy |\n",
    "| **모델 변화** | 가중치 업데이트 됨 | 가중치 고정 |\n",
    "| **목적** | 학습 | 성능 측정 |\n",
    "\n",
    "##### 메모리 사용량 비교\n",
    "\n",
    "```python\n",
    "# 학습 시 (gradient 계산)\n",
    "메모리 = 모델 파라미터 + 활성화 + gradient + optimizer 상태\n",
    "       = 4.20 MB + 0.69 MB + 4.20 MB + 8.40 MB\n",
    "       = 17.49 MB\n",
    "\n",
    "# 평가 시 (no_grad)\n",
    "메모리 = 모델 파라미터 + 활성화\n",
    "       = 4.20 MB + 0.69 MB\n",
    "       = 4.89 MB\n",
    "\n",
    "# 약 3.6배 메모리 절약!\n",
    "```\n",
    "\n",
    "##### 속도 비교\n",
    "\n",
    "```python\n",
    "# 학습 시\n",
    "처리 시간 = forward + loss 계산 + backward + optimizer step\n",
    "          = 100ms + 10ms + 150ms + 50ms\n",
    "          = 310ms per batch\n",
    "\n",
    "# 평가 시\n",
    "처리 시간 = forward만\n",
    "          = 100ms per batch\n",
    "\n",
    "# 약 3배 빠름!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 💡 핵심 개념 정리\n",
    "\n",
    "##### ✅ 반드시 기억할 것\n",
    "\n",
    "###### **1. correct = 0**\n",
    "```python\n",
    "누적 카운터 역할\n",
    "전체 테스트 세트에서 맞춘 개수를 세기 위해\n",
    "```\n",
    "\n",
    "###### **2. with torch.no_grad()**\n",
    "```python\n",
    "# 평가 시 필수!\n",
    "# - Gradient 계산 비활성화\n",
    "# - 메모리 절약 (약 3배)\n",
    "# - 속도 향상 (약 3배)\n",
    "# - 코드의 의도 명확히 표현\n",
    "```\n",
    "\n",
    "###### **3. torch.reshape(data, (-1, 784))**\n",
    "```python\n",
    "# MLP는 1D 입력만 받음\n",
    "# (B, 1, 28, 28) → (B, 784)\n",
    "# 학습과 평가 모두 동일하게 필요\n",
    "```\n",
    "\n",
    "###### **4. output.max(1)[1]**\n",
    "```python\n",
    "# [values, indices] = max(1)\n",
    "# indices만 추출 → 예측 클래스\n",
    "# dim=1: 각 샘플의 클래스 간 최댓값\n",
    "```\n",
    "\n",
    "###### **5. preds.eq(label).sum().item()**\n",
    "```python\n",
    "# eq: 비교 (True/False)\n",
    "# sum: True 개수 세기\n",
    "# item: Tensor → Python int\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🎯 실전 팁\n",
    "\n",
    "##### 더 상세한 평가 코드\n",
    "\n",
    "```python\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in test_loader:\n",
    "        data = torch.reshape(data, (-1, 784)).to(device)\n",
    "        output = model(data)\n",
    "        preds = output.max(1)[1]\n",
    "        \n",
    "        correct += preds.eq(label.to(device)).sum().item()\n",
    "        total += label.size(0)  # 배치 크기 누적\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total})\")\n",
    "    # 출력: Accuracy: 95.23% (9523/10000)\n",
    "```\n",
    "\n",
    "##### 클래스별 정확도\n",
    "\n",
    "```python\n",
    "class_correct = [0] * 10\n",
    "class_total = [0] * 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in test_loader:\n",
    "        data = torch.reshape(data, (-1, 784)).to(device)\n",
    "        output = model(data)\n",
    "        preds = output.max(1)[1]\n",
    "        \n",
    "        # 클래스별로 집계\n",
    "        for pred, true_label in zip(preds, label.to(device)):\n",
    "            if pred == true_label:\n",
    "                class_correct[true_label] += 1\n",
    "            class_total[true_label] += 1\n",
    "\n",
    "# 클래스별 출력\n",
    "for i in range(10):\n",
    "    accuracy = 100 * class_correct[i] / class_total[i]\n",
    "    print(f\"Class {i}: {accuracy:.2f}%\")\n",
    "```\n",
    "\n",
    "##### Confusion Matrix\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in test_loader:\n",
    "        data = torch.reshape(data, (-1, 784)).to(device)\n",
    "        output = model(data)\n",
    "        preds = output.max(1)[1]\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(label.numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(cm)\n",
    "# 어떤 클래스가 어떤 클래스로 잘못 분류되는지 확인\n",
    "```\n",
    "\n",
    "이제 모델 평가 코드를 완벽하게 이해하셨습니다! 🎉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1893d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
