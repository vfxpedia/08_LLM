{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image Captioning 기본구조 (CNN + RNN)"
      ],
      "metadata": {
        "id": "Z5rr_39zoVol"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Xafc6CLSoT5A"
      },
      "outputs": [],
      "source": [
        "# 이미지 전처리\n",
        "\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "image = Image.open('arwas-4.webp').convert('RGB')\n",
        "image_tensor = transform(image).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어사전\n",
        "vocab = {\n",
        "    0: \"<pad>\",\n",
        "    1: \"<start>\",\n",
        "    2: \"<end>\",\n",
        "    3: \"a\",\n",
        "    4: \"dog\",\n",
        "    5: \"is\",\n",
        "    6: \"sitting\",\n",
        "    7: \"on\",\n",
        "    8: \"grass\",\n",
        "    9: \"icecream\"\n",
        "}"
      ],
      "metadata": {
        "id": "1icnsRLjqLjB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG 모델 로드 (이미지로부터 특징 추출)\n",
        "\n",
        "from torchvision.models import vgg16\n",
        "import torch\n",
        "\n",
        "vgg = vgg16(pretrained=True).features\n",
        "\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "with torch.no_grad():\n",
        "    features = vgg(image_tensor)\n",
        "    features = features.view(features.size(0), -1).unsqueeze(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0sDni-TvhvY",
        "outputId": "350895fe-b160-4400-9a85-a494ebcac093"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어사전을 토대로 학습 입출력 데이터 생성\n",
        "\n",
        "caption = [1, 3, 4, 5, 6, 7, 8, 9, 2]\n",
        "input_seq = torch.tensor([caption[:-1]])    # end 토큰빼고\n",
        "target_seq = torch.tensor([caption[1:]])    # start 토큰빼고"
      ],
      "metadata": {
        "id": "jULHJ64px6zE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN 계열 Caption 생성 모델 생성 (이미지 특징 + 이전 단어 -> 다음 단어 예측)\n",
        "# 다만 학습을 하더라고 단어사전(미리 지정한) 안에서만 뱉어주기에 항상 잘 될거임\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class CaptionGenerator(nn.Module):\n",
        "    def __init__(self, feature_dim, embed_dim, hidden_dim, vocab_size):\n",
        "        super(CaptionGenerator, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.decoder = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.init_linear = nn.Linear(feature_dim, embed_dim)\n",
        "\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        embedded_features = self.init_linear(features)\n",
        "        embeds = self.embed(captions)\n",
        "        inputs = torch.cat((embedded_features, embeds), dim=1)\n",
        "        hiddens, _ = self.lstm(inputs)\n",
        "        outputs = self.decoder(hiddens)\n",
        "        return outputs[:, 1:, :]    # 첫 timestep은 제외\n"
      ],
      "metadata": {
        "id": "cE8r4Fd3y5Fk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# 모델 학습\n",
        "model = CaptionGenerator(feature_dim=25088, embed_dim=512, hidden_dim=512, vocab_size=len(vocab))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(features, input_seq)\n",
        "    loss = criterion(outputs.squeeze(0), target_seq.squeeze(0))\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "HUN-lU9a2rlD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 예측\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    generated = []\n",
        "    input_word = torch.tensor([[1]])\n",
        "\n",
        "    embed_feat = model.init_linear(features)\n",
        "    hidden = None\n",
        "\n",
        "    for _ in range(10):\n",
        "        embed_input = model.embed(input_word)\n",
        "        lstm_input = torch.cat((embed_feat, embed_input), dim=1) if len(generated) == 0 else embed_input\n",
        "        out, hidden = model.lstm(lstm_input, hidden)\n",
        "        pred = model.decoder(out[:, -1, :])\n",
        "        pred_id = pred.argmax(dim=-1).item()\n",
        "\n",
        "        if pred_id == 2:\n",
        "            break\n",
        "\n",
        "        generated.append(pred_id)\n",
        "        input_word = torch.tensor([[pred_id]])\n",
        "        embed_feat = None\n",
        "\n",
        "    sentence = \" \".join([vocab[idx] for idx in generated])\n",
        "\n",
        "    print(\"생성된 캡션\", sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mfPJySU3PKz",
        "outputId": "227330ab-e847-4a56-c799-faf435b27cf4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "생성된 캡션 a dog is sitting on grass icecream\n"
          ]
        }
      ]
    }
  ]
}