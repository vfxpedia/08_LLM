{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ca4838",
   "metadata": {},
   "source": [
    "# Q-LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc7de9d",
   "metadata": {},
   "source": [
    "### 한국어 QA 시스템 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c8dfb2",
   "metadata": {},
   "source": [
    "##### 0. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d1f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25126d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (0.1.9)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu128)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.11.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install peft datasets transformers hf_transfer bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f77fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install --upgrade pip # 위 명령어 실행 시 오류 발생 시 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffa88f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install typing-extensions==4.7.1 --upgrade # 위 명령어 실행 시 오류 발생 시 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebae25e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install typing_extensions>=4.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274406f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1\n",
    "%env CUDA_LAUNCH_BLOCKING=1\n",
    "%env NCCL_DEBUG=WARN\n",
    "%env CUDA_DEVICE_MAX_CONNECTIONS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3b9b8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "import bitsandbytes as bnb\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfb27c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 사용 장치: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\" 사용 장치: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddea41b",
   "metadata": {},
   "source": [
    "##### 1. 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b975572",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"NCSOFT/Llama-VARCO-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43e9fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit 양자화된 모델 로드를 하기 위한 설정\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                      # 모델의 가중치를 4-bit 양자화 사용\n",
    "    bnb_4bit_quant_type='nf4',              # 양자화 타입, nf4는 4-bit 양자화 타입\n",
    "    bnb_4bit_use_double_quant=True,         # 이중 양자화 사용 여부\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16   # 계산 데이터 타입\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b5cb7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d579843a394428fa01f8eaa263177f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 토크나이저 및 모델 로드\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# base 모델에 양자화를 적용해서 진행\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f56ff624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt, llama 토크나이저는 pad_token이 없음, decoder-only 모델로 만들어졌기 때문에 오류를 방지 하기 위해서 설정해줌\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae541c4",
   "metadata": {},
   "source": [
    "##### 2. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7923f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드\n",
    "dataset = load_dataset('KorQuAD/squad_kor_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64539666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 60407\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 5774\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e308a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '6566495-0-0',\n",
       " 'title': '파우스트_서곡',\n",
       " 'context': '1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.',\n",
       " 'question': '바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?',\n",
       " 'answers': {'text': ['교향곡'], 'answer_start': [54]}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec59f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "    # 입력 데이터 추출 \n",
    "    inputs = [\"질문: \" + q + \"\\n문맥: \" + c for q, c in zip(examples[\"question\"], examples[\"context\"])]\n",
    "    # 정답 데이터 추출\n",
    "    answer_texts = [a[\"text\"][0] if len(a[\"text\"]) > 0 else \"\" for a in examples[\"answers\"]]\n",
    "    # 입력 데이터 토크나이징\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    # 정답 데이터 토크나이징\n",
    "    labels = tokenizer(\n",
    "        answer_texts,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )['input_ids']\n",
    "    # input_ids 기준으로 labels 의 길이를 맞춤\n",
    "    max_length = model_inputs['input_ids'].shape[1]\n",
    "    labels = labels[:, :max_length]\n",
    "\n",
    "    # 패딩된 부분을 -100으로 설정 (loss 계산 시 무시)\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    # 토크나이징 데이터에 정답 데이터 추가\n",
    "    model_inputs['labels'] = labels\n",
    "    # 토크나이징 데이터 반환\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48673417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 전처리 적용\n",
    "train_dataset = dataset['train'].map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)\n",
    "val_dataset = dataset['validation'].map(preprocess_data, batched=True, remove_columns=dataset['validation'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e0ca9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [103194,\n",
       "  52688,\n",
       "  25,\n",
       "  82818,\n",
       "  49706,\n",
       "  105078,\n",
       "  16969,\n",
       "  114450,\n",
       "  112,\n",
       "  102953,\n",
       "  21028,\n",
       "  56069,\n",
       "  41381,\n",
       "  54289,\n",
       "  18918,\n",
       "  118151,\n",
       "  35495,\n",
       "  118947,\n",
       "  18359,\n",
       "  108533,\n",
       "  35495,\n",
       "  26799,\n",
       "  107762,\n",
       "  16969,\n",
       "  20565,\n",
       "  5380,\n",
       "  52688,\n",
       "  127661,\n",
       "  25,\n",
       "  220,\n",
       "  10750,\n",
       "  24,\n",
       "  100392,\n",
       "  82818,\n",
       "  49706,\n",
       "  105078,\n",
       "  16969,\n",
       "  114450,\n",
       "  112,\n",
       "  102953,\n",
       "  21028,\n",
       "  56069,\n",
       "  41381,\n",
       "  54289,\n",
       "  18359,\n",
       "  114489,\n",
       "  118151,\n",
       "  35495,\n",
       "  55925,\n",
       "  109842,\n",
       "  19954,\n",
       "  109882,\n",
       "  13094,\n",
       "  104519,\n",
       "  234,\n",
       "  101103,\n",
       "  117012,\n",
       "  101228,\n",
       "  58232,\n",
       "  17835,\n",
       "  61816,\n",
       "  27796,\n",
       "  105454,\n",
       "  21028,\n",
       "  101999,\n",
       "  104762,\n",
       "  108047,\n",
       "  18359,\n",
       "  108533,\n",
       "  101103,\n",
       "  16969,\n",
       "  118183,\n",
       "  18359,\n",
       "  116253,\n",
       "  121969,\n",
       "  13,\n",
       "  23955,\n",
       "  45618,\n",
       "  21121,\n",
       "  82818,\n",
       "  49706,\n",
       "  105078,\n",
       "  16969,\n",
       "  220,\n",
       "  10750,\n",
       "  23,\n",
       "  116899,\n",
       "  120997,\n",
       "  107712,\n",
       "  108220,\n",
       "  231,\n",
       "  43139,\n",
       "  105178,\n",
       "  66965,\n",
       "  24140,\n",
       "  66965,\n",
       "  18359,\n",
       "  50467,\n",
       "  98272,\n",
       "  110,\n",
       "  34804,\n",
       "  116492,\n",
       "  103292,\n",
       "  122691,\n",
       "  104834,\n",
       "  54780,\n",
       "  102326,\n",
       "  105115,\n",
       "  19954,\n",
       "  36609,\n",
       "  108185,\n",
       "  102621,\n",
       "  104429,\n",
       "  52491,\n",
       "  102477,\n",
       "  25941,\n",
       "  100757,\n",
       "  58260,\n",
       "  236,\n",
       "  254,\n",
       "  116273,\n",
       "  18918,\n",
       "  63207,\n",
       "  110955,\n",
       "  56069,\n",
       "  41381,\n",
       "  54289,\n",
       "  21028,\n",
       "  106213,\n",
       "  66406,\n",
       "  19954,\n",
       "  100994,\n",
       "  103655,\n",
       "  101528,\n",
       "  35495,\n",
       "  108239,\n",
       "  13,\n",
       "  112887,\n",
       "  56069,\n",
       "  29102,\n",
       "  57575,\n",
       "  49508,\n",
       "  102914,\n",
       "  76242,\n",
       "  45780,\n",
       "  223,\n",
       "  105,\n",
       "  21028,\n",
       "  67890,\n",
       "  169,\n",
       "  115594,\n",
       "  17835,\n",
       "  56069,\n",
       "  29102,\n",
       "  120282,\n",
       "  55421,\n",
       "  93851,\n",
       "  102335,\n",
       "  106356,\n",
       "  101353,\n",
       "  13094,\n",
       "  78453,\n",
       "  55430,\n",
       "  44005,\n",
       "  48765,\n",
       "  58260,\n",
       "  228,\n",
       "  254,\n",
       "  108771,\n",
       "  21028,\n",
       "  101999,\n",
       "  104762,\n",
       "  108047,\n",
       "  220,\n",
       "  24,\n",
       "  43144,\n",
       "  18359,\n",
       "  117512,\n",
       "  35495,\n",
       "  101413,\n",
       "  123143,\n",
       "  103185,\n",
       "  126546,\n",
       "  84696,\n",
       "  101760,\n",
       "  103170,\n",
       "  11,\n",
       "  122182,\n",
       "  13094,\n",
       "  23955,\n",
       "  21819,\n",
       "  105,\n",
       "  34983,\n",
       "  220,\n",
       "  16,\n",
       "  100551,\n",
       "  19954,\n",
       "  56069,\n",
       "  41381,\n",
       "  54289,\n",
       "  21028,\n",
       "  90960,\n",
       "  108047,\n",
       "  43139,\n",
       "  108533,\n",
       "  58126,\n",
       "  86351,\n",
       "  23955,\n",
       "  118999,\n",
       "  19954,\n",
       "  119646,\n",
       "  103292,\n",
       "  49085,\n",
       "  126652,\n",
       "  104519,\n",
       "  120,\n",
       "  106631,\n",
       "  97,\n",
       "  34609,\n",
       "  29102,\n",
       "  107739,\n",
       "  110005,\n",
       "  101787,\n",
       "  102612,\n",
       "  48936,\n",
       "  84618,\n",
       "  122877,\n",
       "  118318,\n",
       "  13,\n",
       "  116864,\n",
       "  21028,\n",
       "  103959,\n",
       "  101353,\n",
       "  93917,\n",
       "  66610,\n",
       "  33931,\n",
       "  21028,\n",
       "  50152,\n",
       "  109018,\n",
       "  109997,\n",
       "  57519,\n",
       "  109509,\n",
       "  103607,\n",
       "  113048,\n",
       "  65621,\n",
       "  72208,\n",
       "  107335,\n",
       "  103123,\n",
       "  106869,\n",
       "  24486,\n",
       "  123323,\n",
       "  82068,\n",
       "  104064,\n",
       "  117465,\n",
       "  102326,\n",
       "  21028,\n",
       "  20565,\n",
       "  64857,\n",
       "  101090,\n",
       "  53400,\n",
       "  105512,\n",
       "  114607,\n",
       "  48765,\n",
       "  58260,\n",
       "  228,\n",
       "  254,\n",
       "  108771,\n",
       "  21028,\n",
       "  106977,\n",
       "  106646,\n",
       "  100848,\n",
       "  104762,\n",
       "  108047,\n",
       "  66610,\n",
       "  33931,\n",
       "  21028,\n",
       "  126652,\n",
       "  84696,\n",
       "  34804,\n",
       "  107387,\n",
       "  110773,\n",
       "  29833,\n",
       "  91786,\n",
       "  13,\n",
       "  117699,\n",
       "  101999,\n",
       "  104762,\n",
       "  108047,\n",
       "  69332,\n",
       "  108047,\n",
       "  18359,\n",
       "  220,\n",
       "  10750,\n",
       "  24,\n",
       "  100392,\n",
       "  103551,\n",
       "  220,\n",
       "  1272,\n",
       "  116899,\n",
       "  105701,\n",
       "  110218,\n",
       "  56069,\n",
       "  29102,\n",
       "  57575,\n",
       "  122787,\n",
       "  24140,\n",
       "  102621,\n",
       "  112804,\n",
       "  220,\n",
       "  16,\n",
       "  106356,\n",
       "  115096,\n",
       "  120657,\n",
       "  112,\n",
       "  107333,\n",
       "  19954,\n",
       "  72043,\n",
       "  101353,\n",
       "  101528,\n",
       "  13,\n",
       "  112887,\n",
       "  118999,\n",
       "  21028,\n",
       "  107123,\n",
       "  33931,\n",
       "  54780,\n",
       "  101604,\n",
       "  118472,\n",
       "  108154,\n",
       "  23955,\n",
       "  90960,\n",
       "  108047,\n",
       "  7,\n",
       "  16,\n",
       "  106356,\n",
       "  41953,\n",
       "  122369,\n",
       "  56069,\n",
       "  29102,\n",
       "  120282,\n",
       "  123645,\n",
       "  78453,\n",
       "  55430,\n",
       "  62841,\n",
       "  57575,\n",
       "  78453,\n",
       "  55430,\n",
       "  48936,\n",
       "  56069,\n",
       "  29726,\n",
       "  42771,\n",
       "  102704,\n",
       "  119225,\n",
       "  111320,\n",
       "  112804,\n",
       "  11,\n",
       "  122499,\n",
       "  126367,\n",
       "  120893,\n",
       "  32179,\n",
       "  22035,\n",
       "  107054,\n",
       "  112269,\n",
       "  13,\n",
       "  83719,\n",
       "  100654,\n",
       "  84415,\n",
       "  101347,\n",
       "  34804,\n",
       "  220,\n",
       "  19,\n",
       "  100392,\n",
       "  22817,\n",
       "  63718,\n",
       "  121066,\n",
       "  95415,\n",
       "  19954,\n",
       "  103745,\n",
       "  116273,\n",
       "  69697,\n",
       "  112,\n",
       "  57575,\n",
       "  78453,\n",
       "  55430,\n",
       "  109791,\n",
       "  35495,\n",
       "  102888,\n",
       "  101347,\n",
       "  49085,\n",
       "  120893,\n",
       "  32179,\n",
       "  106872,\n",
       "  102077,\n",
       "  11,\n",
       "  111323,\n",
       "  19954,\n",
       "  55925,\n",
       "  106687,\n",
       "  75908,\n",
       "  60798,\n",
       "  116039,\n",
       "  101264,\n",
       "  103211,\n",
       "  13,\n",
       "  55925,\n",
       "  109055,\n",
       "  19954,\n",
       "  108154,\n",
       "  58083,\n",
       "  108733,\n",
       "  60798,\n",
       "  81673,\n",
       "  75908,\n",
       "  104911,\n",
       "  44005,\n",
       "  103315,\n",
       "  69697,\n",
       "  250,\n",
       "  103272,\n",
       "  30446,\n",
       "  123486,\n",
       "  107123,\n",
       "  33931,\n",
       "  101360,\n",
       "  120878,\n",
       "  48424,\n",
       "  13094,\n",
       "  101464,\n",
       "  109018,\n",
       "  122787,\n",
       "  24140,\n",
       "  44005,\n",
       "  78102,\n",
       "  101968,\n",
       "  55430,\n",
       "  24486,\n",
       "  106243,\n",
       "  18359,\n",
       "  64432,\n",
       "  103222,\n",
       "  230,\n",
       "  103170,\n",
       "  11,\n",
       "  108542,\n",
       "  82818,\n",
       "  113222,\n",
       "  250,\n",
       "  120376,\n",
       "  13094,\n",
       "  23955,\n",
       "  46230,\n",
       "  94,\n",
       "  18359,\n",
       "  16633,\n",
       "  232,\n",
       "  58901,\n",
       "  62398,\n",
       "  105512,\n",
       "  116548,\n",
       "  20565,\n",
       "  105365,\n",
       "  101787,\n",
       "  107621,\n",
       "  49085,\n",
       "  91786,\n",
       "  13,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [100848,\n",
       "  104762,\n",
       "  108047,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e4cb886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?\n",
      "문맥: 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_dataset[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db9cb04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?\n",
      "문맥: 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_dataset[0]['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b2dc3",
   "metadata": {},
   "source": [
    "##### 3. 모델 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2d726d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,                                    # 모델 학습 시 사용되는 모듈의 수, 저차원의 rank 수\n",
    "    lora_alpha=32,                          # lora_alpha 값이 클수록 모델 학습 시 사용되는 모듈의 수가 많아짐\n",
    "    lora_dropout=0.1,                       # 드롭아웃 비율, 과적합 방지\n",
    "    bias='none',                            # 편향 사용 여부\n",
    "    target_modules=['q_proj', 'v_proj'],    # 학습할 모듈 지정, 대상이 될 모듈 지정 ('q_proj': 쿼리 프로젝션, 'v_proj': 값 프로젝션)\n",
    "    task_type='CAUSAL_LM'                   # 학습 타입 지정 (Causal Language Model: 예측 모델)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe472c7",
   "metadata": {},
   "source": [
    "##### target_modules\n",
    "|  옵션  | 설명 |\n",
    "|:---------------------:|:--------------------------------------------------------------|\n",
    "| `'q_proj'`            | Attention의 Query projection. 주로 효율과 성능의 균형을 위해 기본적으로 선택됨. |\n",
    "| `'v_proj'`            | Attention의 Value projection. 모델 성능 최적화 시, 특히 대형 모델에 권장됨. |\n",
    "| `'k_proj'`            | Attention의 Key projection. Value projection과 함께 성능 최적화 및 대형 모델에 적합. |\n",
    "| `'o_proj'`            | Attention Output projection. 성능 향상을 위해 대형 모델에서 사용 가능. |\n",
    "| `'gate_proj'`         | 게이트 프로젝션. 게이트 구조와 결합하여 추가적인 성능 개선 가능. |\n",
    "| `'down_proj'`         | 다운 프로젝션. 차원 축소 등 특정 구조에서 성능 및 효율 개선을 위해 활용. |\n",
    "| `'up_proj'`           | 업 프로젝션. 차원 확장 구조에서 성능 향상 혹은 모델 구조 의도에 따라 적용. |\n",
    "\n",
    "---\n",
    "##### task_type\n",
    "|  옵션      | 설명 |\n",
    "|:--------------------:|:--------------------------------------------------------------|\n",
    "| `'CAUSAL_LM'`        | 입력 기반 다음 토큰 예측 (언어모델링/텍스트 생성) 태스크에 사용. |\n",
    "| `'SEQ_2_SEQ_LM'`     | 입력 시퀀스 → 출력 시퀀스 생성(예: 번역, 요약 등) 태스크에 사용.      |\n",
    "| `'SEQ_CLS'`          | 시퀀스 전체 분류(예: 감정 분류, 스팸 분류 등) 태스크에 사용.          |\n",
    "| `'TOKEN_CLS'`        | 각 토큰별 클래스를 예측(예: 개체명 인식) 태스크에 사용.              |\n",
    "| `'QUESTION_ANSWERING'` | 질문+문맥이 주어질 때 답 예측(질의응답) 태스크에 사용.           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa86aa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "# 모델에 lora 적용\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "# 입력 데이터에 대한 그래디언트 계산 활성화\n",
    "model.enable_input_require_grads()\n",
    "# 그래디언트 체크포인팅 활성화\n",
    "model.gradient_checkpointing_enable()\n",
    "# 학습 가능한 파라미터 출력\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a176d5",
   "metadata": {},
   "source": [
    "```python\n",
    "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n",
    "```\n",
    "\n",
    "위 결과는 현재 로라(LoRA) 방법으로 학습하는 파라미터(가중치)의 개수와 전체 모델 파라미터 중 학습 가능 파라미터가 차지하는 비율을 보여줍니다.\n",
    "\n",
    "| 항목                | 값 (예시)           | 설명                                                                              |\n",
    "|---------------------|---------------------|-----------------------------------------------------------------------------------|\n",
    "| trainable params    | 3,407,872           | 실제로 학습이 진행되는 파라미터 개수.<br>약 340만 개만 업데이트 및 미세조정 됨.      |\n",
    "| all params          | 8,033,669,120       | 전체 사전학습 모델의 파라미터 총 개수.<br>대부분은 고정(freeze)되어 있음.           |\n",
    "| trainable %         | 0.0424%             | 전체 파라미터 중 실제 학습이 적용되는 비율.<br>아주 작은 일부만 미세조정 됨.         |\n",
    "\n",
    "**설명**\n",
    "- LoRA와 같은 방식은 전체 대용량 파라미터 중 극히 일부만 효율적으로 학습하여, 연산 자원과 메모리를 크게 절약할 수 있음.\n",
    "- 우리 코드에서는 학습률(learning rate)을 1%로 주어, 학습 대상 파라미터가 빠르게 수렴하도록 설정함.\n",
    "- LoRA 등 파라미터 효율 방식은 학습률을 비교적 높게 줄 때 효과적이며, 빠른 실험과 수렴에 유리함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3ac3293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataCollatorForSeq2Seq 초기화 → 데이터 셋 전처리 과정에서 사용되는 클래스\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a256e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./q_lora_korqa',\n",
    "    save_strategy='epoch',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8, # batch size가 작을때 누산해서 학습을 효과적으로 진행하는 방법\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,             # 허깅페이스 허락 없이 푸시 하면 오류 발생\n",
    "    report_to='none'               # report 없이 진행\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d46dc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 프로세스 초기화\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    # tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec514135",
   "metadata": {},
   "source": [
    "##### 4. 모델 학습(Fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e2fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31423c1",
   "metadata": {},
   "source": [
    "##### 5. 학습된 모델 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b30a8f0",
   "metadata": {},
   "source": [
    "(1) 모델 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7400ca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정 저장\n",
    "from transformers import AutoConfig\n",
    "\n",
    "trained_model_path = './q_lora_korqa/checkpoint-1888'\n",
    "# 사전학습된 모델 객체의 정보를 불러온다. 기반 모델의 정보를 불러와서 fine tuning 된 모델 경로에 저장해준다.\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.save_pretrained(trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76264dc-bf42-4cd4-bc39-52ca94ebf964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6f0c7c702d4508883b349f51fa09b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The model has been loaded with `accelerate` and therefore cannot be moved to a specific device. Please discard the `device` argument when creating your pipeline object.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     25\u001b[39m model = PeftModel.from_pretrained(\n\u001b[32m     26\u001b[39m     base_model,\n\u001b[32m     27\u001b[39m     trained_model_path,\n\u001b[32m     28\u001b[39m     torch_dtype=torch.bfloat16\n\u001b[32m     29\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 4) pipeline 정의 시 device 고정\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m qa_pipeline = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# 첫 번째 A40에 고정\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/__init__.py:1229\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1227\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mprocessor\u001b[39m\u001b[33m\"\u001b[39m] = processor\n\u001b[32m-> \u001b[39m\u001b[32m1229\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_generation.py:121\u001b[39m, in \u001b[36mTextGenerationPipeline.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_model_type(\n\u001b[32m    123\u001b[39m         TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n\u001b[32m    124\u001b[39m     )\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mprefix\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._preprocess_params:\n\u001b[32m    126\u001b[39m         \u001b[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001b[39;00m\n\u001b[32m    127\u001b[39m         \u001b[38;5;66;03m# as a \"default\".\u001b[39;00m\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# which is why we cannot put them in their respective methods.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py:981\u001b[39m, in \u001b[36mPipeline.__init__\u001b[39m\u001b[34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, device, binary_output, **kwargs)\u001b[39m\n\u001b[32m    978\u001b[39m hf_device_map = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mhf_device_map\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m981\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    982\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe model has been loaded with `accelerate` and therefore cannot be moved to a specific device. Please \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    983\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdiscard the `device` argument when creating your pipeline object.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    984\u001b[39m     )\n\u001b[32m    986\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    987\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    988\u001b[39m         \u001b[38;5;66;03m# Take the first device used by `accelerate`.\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: The model has been loaded with `accelerate` and therefore cannot be moved to a specific device. Please discard the `device` argument when creating your pipeline object."
     ]
    }
   ],
   "source": [
    "# 베이스 모델 학습 결과\n",
    "from peft import PeftModel\n",
    "from transformers import pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# --- 중요 ---\n",
    "# 훈련 시 사용했던 4-bit 양자화 설정을 *그대로* 다시 정의해야 합니다.\n",
    "# (노트북 세션이 재시작되었다면 quant_config 객체가 없을 수 있으므로)\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 2) 베이스 모델을 16bit로 로드\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,   # 또는 torch.float16\n",
    "    device_map='auto',\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# 3) LoRA 로드\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    trained_model_path,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 4) pipeline 정의 시 device 고정\n",
    "qa_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b847cb7c-898d-4530-bef9-4a4831ae24a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3878de-31a3-4858-9102-6d5d4bf68025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question = '바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?'\n",
    "context = '1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.'\n",
    "\n",
    "# 수정 input_text\n",
    "input_text = f\"질문: {question}\\n문맥: {context}\\n답변:\" # <-- 답변 시작을 유도합니다.\n",
    "\n",
    "output = qa_pipeline(\n",
    "    input_text,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=False\n",
    ")\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e477cd",
   "metadata": {},
   "source": [
    "(2) HuggingFace Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa5ee60",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b150113-40d9-424f-aaa0-bbc3e2337f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
