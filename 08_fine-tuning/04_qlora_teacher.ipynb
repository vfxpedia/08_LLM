{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12336cc6",
   "metadata": {},
   "source": [
    "# Q-LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259e6d69",
   "metadata": {},
   "source": [
    "### 한국어 QA 시스템 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad17e34a",
   "metadata": {},
   "source": [
    "##### 0. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d54baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft datasets transformers hf_transfer bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5437f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fdc15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install typing_extensions==4.7.1 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fa11a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq, AutoConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6703678",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11cb1a2",
   "metadata": {},
   "source": [
    "##### 1. 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d43868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'NCSOFT/Llama-VARCO-8B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df565c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb060662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 및 모델 로드\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63870edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae84aaf0",
   "metadata": {},
   "source": [
    "##### 2. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a6dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드\n",
    "dataset = load_dataset('KorQuAD/squad_kor_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c8589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af17a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "    \n",
    "    inputs = [\"질문: \" + q + \"\\n문맥: \" + c for q, c in zip(examples[\"question\"], examples[\"context\"])]\n",
    "    answer_texts = [a[\"text\"][0] if len(a[\"text\"]) > 0 else \"\" for a in examples[\"answers\"]]\n",
    "\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        answer_texts,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )['input_ids']\n",
    "    \n",
    "    max_length = model_inputs['input_ids'].shape[1]\n",
    "    labels = labels[:, :max_length]\n",
    "\n",
    "\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    model_inputs['labels'] = labels\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a72d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 전처리 적용\n",
    "train_dataset = dataset['train'].map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names) \n",
    "val_dataset = dataset['validation'].map(preprocess_data, batched=True, remove_columns=dataset['validation'].column_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c13b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee2177",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(train_dataset[0]['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c0131",
   "metadata": {},
   "source": [
    "##### 3. 모델 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ede5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias='none',\n",
    "    target_modules=['q_proj', 'v_proj'],\n",
    "    task_type='CAUSAL_LM'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f79da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "model.enable_input_require_grads()\n",
    "model.gradient_checkpointing_enable()\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0672d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a2460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./q_lora_korqa',\n",
    "    save_strategy='epoch',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    report_to='none'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42be7c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "model=model,\n",
    "args=training_args,\n",
    "train_dataset=train_dataset,\n",
    "eval_dataset=val_dataset,\n",
    "tokenizer=tokenizer,\n",
    "data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7aee60",
   "metadata": {},
   "source": [
    "##### 4. 모델 학습 (Fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddaa32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1af3637",
   "metadata": {},
   "source": [
    "##### 5. 학습된 모델 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8719d76",
   "metadata": {},
   "source": [
    "(1) 모델 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ae3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "trained_model_path = './q_lora_korqa/checkpoint-1888'\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.save_pretrained(trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097df24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import pipeline\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype='auto', device_map='auto')\n",
    "model = PeftModel.from_pretrained(base_model, trained_model_path)\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41acd146",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?'\n",
    "context = '1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.'\n",
    "\n",
    "input_text = f\"질문: {question}\\n문맥: {context}\"\n",
    "\n",
    "output = qa_pipeline(input_text, max_new_tokens=50, temperature=0.2, top_p=0.8)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e2ccc",
   "metadata": {},
   "source": [
    "(2) HuggingFace Hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f80d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./q_lora_korqa')\n",
    "tokenizer.save_pretrained('./q_lora_korqa')\n",
    "\n",
    "model.push_to_hub('codespace01/q_lora_korqa')\n",
    "tokenizer.push_to_hub('codespace01/q_lora_korqa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532575ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"NCSOFT/Llama-VARCO-8B-Instruct\",\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    'codespace01/q_lora_korqa'\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('codespace01/q_lora_korqa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd543e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?', return_tensors='pt').to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
