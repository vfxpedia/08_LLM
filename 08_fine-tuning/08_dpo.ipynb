{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec9741c8",
   "metadata": {},
   "source": [
    "# DPO (Direct Preference Optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a73b3",
   "metadata": {},
   "source": [
    "### 0. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5b704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!pip install typing_extensions==4.7.1 --upgrade\n",
    "!pip install transformers peft datasets bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "171041da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6b3cbd",
   "metadata": {},
   "source": [
    "### 1. 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "742ea784",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Bllossom/llama-3.2-Korean-Bllossom-3B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b756f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b61ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Exception in thread Thread-44 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Playdata\\anaconda3\\envs\\llm_env\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\Playdata\\anaconda3\\envs\\llm_env\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\Playdata\\anaconda3\\envs\\llm_env\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "  File \"<frozen codecs>\", line 322, in decode\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc0 in position 6: invalid start byte\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e438f515394c3d851cd9b46d9767e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85e8bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34305337",
   "metadata": {},
   "source": [
    "### 2. 학습 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b3b00",
   "metadata": {},
   "source": [
    "(1) 모델 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b353ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d195a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,156,928 || all params: 3,224,906,752 || trainable%: 0.3770\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "model.train()\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b1ce1e",
   "metadata": {},
   "source": [
    "(2) 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "159857f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624856122b84416e949acab688fe26b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\datasets--mncai--orca_dpo_pairs_ko. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad2b98b31bde4067bc070cdaa0bb3721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "orca_dpo_pairs_ko.jsonl:   0%|          | 0.00/34.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ee04fbcb0b4e93bab939e22fa0f433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10556 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('mncai/orca_dpo_pairs_ko')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff5cf44",
   "metadata": {},
   "source": [
    "(3) 전처리 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0b65ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sample):\n",
    "    input_enc = tokenizer(sample[\"question\"], padding=\"max_length\", max_length=256, truncation=True)\n",
    "    preferred_enc = tokenizer(sample[\"chosen\"], padding=\"max_length\", max_length=256, truncation=True)\n",
    "    despreferred_enc = tokenizer(sample[\"rejected\"], padding=\"max_length\", max_length=256, truncation=True)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_enc[\"input_ids\"],\n",
    "        \"attention_mask\": input_enc[\"attention_mask\"],\n",
    "        \"preferred_ids\": preferred_enc[\"input_ids\"],\n",
    "        \"despreferred_ids\": despreferred_enc[\"input_ids\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef782b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb87bcb02537404f86179228b3322d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10556 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset['train'].map(\n",
    "    preprocess_text,\n",
    "    remove_columns=['id', 'system', 'question', 'chosen', 'rejected']\n",
    ")\n",
    "\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'preferred_ids', 'despreferred_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fb19d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item[\"input_ids\"].clone().detach() for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"].clone().detach() for item in batch])\n",
    "    \n",
    "    max_length = max(max(len(item['preferred_ids']) for item in batch), 1)\n",
    "\n",
    "    preferred_ids = torch.stack([\n",
    "        torch.tensor(\n",
    "            item[\"preferred_ids\"].tolist() + [tokenizer.pad_token_id] * (max_length - len(item['preferred_ids'])),\n",
    "            dtype=torch.long\n",
    "        ) if isinstance(item[\"preferred_ids\"], torch.Tensor) else\n",
    "        torch.tensor(\n",
    "            item[\"preferred_ids\"] + [tokenizer.pad_token_id] * (max_length - len(item[\"preferred_ids\"])),\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        for item in batch\n",
    "    ]).clone().detach()\n",
    "\n",
    "    despreferred_ids = torch.stack([\n",
    "        torch.tensor(\n",
    "            item[\"despreferred_ids\"].tolist() + [tokenizer.pad_token_id] * (max_length - len(item['despreferred_ids'])),\n",
    "            dtype=torch.long\n",
    "        ) if isinstance(item[\"despreferred_ids\"], torch.Tensor) else\n",
    "        torch.tensor(\n",
    "            item[\"despreferred_ids\"] + [tokenizer.pad_token_id] * (max_length - len(item[\"despreferred_ids\"])),\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        for item in batch\n",
    "    ]).clone().detach()\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"preferred_ids\": preferred_ids,\n",
    "        \"despreferred_ids\": despreferred_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe64599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DTOTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, beta=0.1, *args, **kwargs):\n",
    "        input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "        preferred_ids = inputs[\"preferred_ids\"].to(model.device)\n",
    "        despreferred_ids = inputs[\"despreferred_ids\"].to(model.device)\n",
    "\n",
    "        preferred_outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=preferred_ids)\n",
    "        despreferred_outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=despreferred_ids)\n",
    "        \n",
    "        preferred_loss = preferred_outputs.loss\n",
    "        despreferred_loss = despreferred_outputs.loss\n",
    "\n",
    "        loss = -F.logsigmoid(beta * (despreferred_loss - preferred_loss)).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b5a8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "checkpoint_path = \"./dpo_llama3_korean/checkpoint-xxx\"\n",
    "\n",
    "model = PeftModel.from_pretrained(model, checkpoint_path)\n",
    "model.eval()\n",
    "\n",
    "sample_data = dataset[\"train\"].select(range(5))\n",
    "\n",
    "def generate_response(question):\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=256,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    \n",
    " \n",
    " for i, example in enumerate(sample_data):\n",
    "    question = example[\"question\"]\n",
    "    preferred_answer = example[\"chosen\"]\n",
    "\n",
    "    generated_reseponse = generate_response(question)\n",
    "\n",
    "    print(f\"{i}번째 질문: {question}\")\n",
    "    print(f\"정답 (선호 응답): {preferred_answer}\")\n",
    "    print(f\"실제 모델 응답: {generated_reseponse}\")\n",
    "    print(\"=\" * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
