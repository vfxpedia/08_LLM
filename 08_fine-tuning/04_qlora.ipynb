{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ca4838",
   "metadata": {},
   "source": [
    "# Q-LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc7de9d",
   "metadata": {},
   "source": [
    "### 한국어 QA 시스템 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c8dfb2",
   "metadata": {},
   "source": [
    "##### 0. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d1f9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing_extensions in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (4.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25126d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: hf_transfer in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (0.1.9)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (0.48.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from peft) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from peft) (2.9.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from peft) (1.11.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from peft) (0.36.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from torch>=1.13.0->peft) (80.9.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\playdata\\anaconda3\\envs\\llm_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install peft datasets transformers hf_transfer bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75f77fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install --upgrade pip # 위 명령어 실행 시 오류 발생 시 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ffa88f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install typing-extensions==4.7.1 --upgrade # 위 명령어 실행 시 오류 발생 시 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebae25e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install typing_extensions>=4.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3b9b8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Playdata\\anaconda3\\envs\\llm_env\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\Playdata\\anaconda3\\envs\\llm_env\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\Playdata\\anaconda3\\envs\\llm_env\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "  File \"<frozen codecs>\", line 322, in decode\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc0 in position 6: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "import bitsandbytes as bnb\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfb27c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 사용 장치: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\" 사용 장치: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddea41b",
   "metadata": {},
   "source": [
    "##### 1. 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b975572",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"NCSOFT/Llama-VARCO-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e9fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit 양자화된 모델 로드를 하기 위한 설정\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                      # 모델의 가중치를 4-bit 양자화 사용\n",
    "    bnb_4bit_quant_type='nf4',              # 양자화 타입, nf4는 4-bit 양자화 타입\n",
    "    bnb_4bit_use_double_quant=True,         # 이중 양자화 사용 여부\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16   # 계산 데이터 타입\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5cb7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b2d9c863a34e6a93c098e827346fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--NCSOFT--Llama-VARCO-8B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2421e7d25d49401aa536bc304d66c3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a89cb719be40a5ba87ca9cf4bfa3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/430 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fe2f5c3ad14c9fa2b927ab10fdea77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292066ef467447b8b35ff62fd041a9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d9fc932e2142f690b27aa64be97b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa0fe51c2484e71b7a15c53ca809fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a17a531e2c44a8bae86d9c69592a915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b12f7e2a234fbd9267bec889afb03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c21df703cd8449dbec161332f3316e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a35695d9fe874c33acac8e3c3fcf7c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 토크나이저 및 모델 로드\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# base 모델에 양자화를 적용해서 진행\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f56ff624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt, llama 토크나이저는 pad_token이 없음, decoder-only 모델로 만들어졌기 때문에 오류를 방지 하기 위해서 설정해줌\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae541c4",
   "metadata": {},
   "source": [
    "##### 2. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7923f858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c001308facb24fa1900cb8c9d08c52e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\datasets--KorQuAD--squad_kor_v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b65a783bc747c0ab25545e4921c67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "squad_kor_v1/train-00000-of-00001.parque(…):   0%|          | 0.00/11.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de2375fec1e4d78801ebafaa325b5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "squad_kor_v1/validation-00000-of-00001.p(…):   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc91d00ea834de39a416385cd1fa440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/60407 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78b23fd5195455d9d7012a79d49838b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/5774 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터셋 로드\n",
    "dataset = load_dataset('KorQuAD/squad_kor_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64539666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 60407\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 5774\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e308a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '6566495-0-0',\n",
       " 'title': '파우스트_서곡',\n",
       " 'context': '1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.',\n",
       " 'question': '바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?',\n",
       " 'answers': {'text': ['교향곡'], 'answer_start': [54]}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec59f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "    # 입력 데이터 추출 \n",
    "    inputs = [\"질문: \" + q + \"\\n문맥: \" + c for q, c in zip(examples[\"question\"], examples[\"context\"])]\n",
    "    # 정답 데이터 추출\n",
    "    answer_texts = [a[\"text\"][0] if len(a[\"text\"]) > 0 else \"\" for a in examples[\"answers\"]]\n",
    "    # 입력 데이터 토크나이징\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    # 정답 데이터 토크나이징\n",
    "    labels = tokenizer(\n",
    "        answer_texts,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )['input_ids']\n",
    "    # input_ids 기준으로 labels 의 길이를 맞춤\n",
    "    max_length = model_inputs['input_ids'].shape[1]\n",
    "    labels = labels[:, :max_length]\n",
    "\n",
    "    # 패딩된 부분을 -100으로 설정 (loss 계산 시 무시)\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    # 토크나이징 데이터에 정답 데이터 추가\n",
    "    model_inputs['labels'] = labels\n",
    "    # 토크나이징 데이터 반환\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48673417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5e4e2b989e403c906213531b947f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60407 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00918e7c219f46cabdf34ff75836eb56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5774 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터셋 전처리 적용\n",
    "train_dataset = dataset['train'].map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)\n",
    "val_dataset = dataset['validation'].map(preprocess_data, batched=True, remove_columns=dataset['validation'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e0ca9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [103194,\n",
       "  52688,\n",
       "  25,\n",
       "  82818,\n",
       "  49706,\n",
       "  105078,\n",
       "  16969,\n",
       "  114450,\n",
       "  112,\n",
       "  102953,\n",
       "  21028,\n",
       "  56069,\n",
       "  41381,\n",
       "  54289,\n",
       "  18918,\n",
       "  118151,\n",
       "  35495,\n",
       "  118947,\n",
       "  18359,\n",
       "  108533,\n",
       "  35495,\n",
       "  26799,\n",
       "  107762,\n",
       "  16969,\n",
       "  20565,\n",
       "  5380,\n",
       "  52688,\n",
       "  127661,\n",
       "  25,\n",
       "  220,\n",
       "  10750,\n",
       "  24,\n",
       "  100392,\n",
       "  82818,\n",
       "  49706,\n",
       "  105078,\n",
       "  16969,\n",
       "  114450,\n",
       "  112,\n",
       "  102953,\n",
       "  21028,\n",
       "  56069,\n",
       "  41381,\n",
       "  54289,\n",
       "  18359,\n",
       "  114489,\n",
       "  118151,\n",
       "  35495,\n",
       "  55925,\n",
       "  109842,\n",
       "  19954,\n",
       "  109882,\n",
       "  13094,\n",
       "  104519,\n",
       "  234,\n",
       "  101103,\n",
       "  117012,\n",
       "  101228,\n",
       "  58232,\n",
       "  17835,\n",
       "  61816,\n",
       "  27796,\n",
       "  105454,\n",
       "  21028,\n",
       "  101999,\n",
       "  104762,\n",
       "  108047,\n",
       "  18359,\n",
       "  108533,\n",
       "  101103,\n",
       "  16969,\n",
       "  118183,\n",
       "  18359,\n",
       "  116253,\n",
       "  121969,\n",
       "  13,\n",
       "  23955,\n",
       "  45618,\n",
       "  21121,\n",
       "  82818,\n",
       "  49706,\n",
       "  105078,\n",
       "  16969,\n",
       "  220,\n",
       "  10750,\n",
       "  23,\n",
       "  116899,\n",
       "  120997,\n",
       "  107712,\n",
       "  108220,\n",
       "  231,\n",
       "  43139,\n",
       "  105178,\n",
       "  66965,\n",
       "  24140,\n",
       "  66965,\n",
       "  18359,\n",
       "  50467,\n",
       "  98272,\n",
       "  110,\n",
       "  34804,\n",
       "  116492,\n",
       "  103292,\n",
       "  122691,\n",
       "  104834,\n",
       "  54780,\n",
       "  102326,\n",
       "  105115,\n",
       "  19954,\n",
       "  36609,\n",
       "  108185,\n",
       "  102621,\n",
       "  104429,\n",
       "  52491,\n",
       "  102477,\n",
       "  25941,\n",
       "  100757,\n",
       "  58260,\n",
       "  236,\n",
       "  254,\n",
       "  116273,\n",
       "  18918,\n",
       "  63207,\n",
       "  110955,\n",
       "  56069,\n",
       "  41381,\n",
       "  54289,\n",
       "  21028,\n",
       "  106213,\n",
       "  66406,\n",
       "  19954,\n",
       "  100994,\n",
       "  103655,\n",
       "  101528,\n",
       "  35495,\n",
       "  108239,\n",
       "  13,\n",
       "  112887,\n",
       "  56069,\n",
       "  29102,\n",
       "  57575,\n",
       "  49508,\n",
       "  102914,\n",
       "  76242,\n",
       "  45780,\n",
       "  223,\n",
       "  105,\n",
       "  21028,\n",
       "  67890,\n",
       "  169,\n",
       "  115594,\n",
       "  17835,\n",
       "  56069,\n",
       "  29102,\n",
       "  120282,\n",
       "  55421,\n",
       "  93851,\n",
       "  102335,\n",
       "  106356,\n",
       "  101353,\n",
       "  13094,\n",
       "  78453,\n",
       "  55430,\n",
       "  44005,\n",
       "  48765,\n",
       "  58260,\n",
       "  228,\n",
       "  254,\n",
       "  108771,\n",
       "  21028,\n",
       "  101999,\n",
       "  104762,\n",
       "  108047,\n",
       "  220,\n",
       "  24,\n",
       "  43144,\n",
       "  18359,\n",
       "  117512,\n",
       "  35495,\n",
       "  101413,\n",
       "  123143,\n",
       "  103185,\n",
       "  126546,\n",
       "  84696,\n",
       "  101760,\n",
       "  103170,\n",
       "  11,\n",
       "  122182,\n",
       "  13094,\n",
       "  23955,\n",
       "  21819,\n",
       "  105,\n",
       "  34983,\n",
       "  220,\n",
       "  16,\n",
       "  100551,\n",
       "  19954,\n",
       "  56069,\n",
       "  41381,\n",
       "  54289,\n",
       "  21028,\n",
       "  90960,\n",
       "  108047,\n",
       "  43139,\n",
       "  108533,\n",
       "  58126,\n",
       "  86351,\n",
       "  23955,\n",
       "  118999,\n",
       "  19954,\n",
       "  119646,\n",
       "  103292,\n",
       "  49085,\n",
       "  126652,\n",
       "  104519,\n",
       "  120,\n",
       "  106631,\n",
       "  97,\n",
       "  34609,\n",
       "  29102,\n",
       "  107739,\n",
       "  110005,\n",
       "  101787,\n",
       "  102612,\n",
       "  48936,\n",
       "  84618,\n",
       "  122877,\n",
       "  118318,\n",
       "  13,\n",
       "  116864,\n",
       "  21028,\n",
       "  103959,\n",
       "  101353,\n",
       "  93917,\n",
       "  66610,\n",
       "  33931,\n",
       "  21028,\n",
       "  50152,\n",
       "  109018,\n",
       "  109997,\n",
       "  57519,\n",
       "  109509,\n",
       "  103607,\n",
       "  113048,\n",
       "  65621,\n",
       "  72208,\n",
       "  107335,\n",
       "  103123,\n",
       "  106869,\n",
       "  24486,\n",
       "  123323,\n",
       "  82068,\n",
       "  104064,\n",
       "  117465,\n",
       "  102326,\n",
       "  21028,\n",
       "  20565,\n",
       "  64857,\n",
       "  101090,\n",
       "  53400,\n",
       "  105512,\n",
       "  114607,\n",
       "  48765,\n",
       "  58260,\n",
       "  228,\n",
       "  254,\n",
       "  108771,\n",
       "  21028,\n",
       "  106977,\n",
       "  106646,\n",
       "  100848,\n",
       "  104762,\n",
       "  108047,\n",
       "  66610,\n",
       "  33931,\n",
       "  21028,\n",
       "  126652,\n",
       "  84696,\n",
       "  34804,\n",
       "  107387,\n",
       "  110773,\n",
       "  29833,\n",
       "  91786,\n",
       "  13,\n",
       "  117699,\n",
       "  101999,\n",
       "  104762,\n",
       "  108047,\n",
       "  69332,\n",
       "  108047,\n",
       "  18359,\n",
       "  220,\n",
       "  10750,\n",
       "  24,\n",
       "  100392,\n",
       "  103551,\n",
       "  220,\n",
       "  1272,\n",
       "  116899,\n",
       "  105701,\n",
       "  110218,\n",
       "  56069,\n",
       "  29102,\n",
       "  57575,\n",
       "  122787,\n",
       "  24140,\n",
       "  102621,\n",
       "  112804,\n",
       "  220,\n",
       "  16,\n",
       "  106356,\n",
       "  115096,\n",
       "  120657,\n",
       "  112,\n",
       "  107333,\n",
       "  19954,\n",
       "  72043,\n",
       "  101353,\n",
       "  101528,\n",
       "  13,\n",
       "  112887,\n",
       "  118999,\n",
       "  21028,\n",
       "  107123,\n",
       "  33931,\n",
       "  54780,\n",
       "  101604,\n",
       "  118472,\n",
       "  108154,\n",
       "  23955,\n",
       "  90960,\n",
       "  108047,\n",
       "  7,\n",
       "  16,\n",
       "  106356,\n",
       "  41953,\n",
       "  122369,\n",
       "  56069,\n",
       "  29102,\n",
       "  120282,\n",
       "  123645,\n",
       "  78453,\n",
       "  55430,\n",
       "  62841,\n",
       "  57575,\n",
       "  78453,\n",
       "  55430,\n",
       "  48936,\n",
       "  56069,\n",
       "  29726,\n",
       "  42771,\n",
       "  102704,\n",
       "  119225,\n",
       "  111320,\n",
       "  112804,\n",
       "  11,\n",
       "  122499,\n",
       "  126367,\n",
       "  120893,\n",
       "  32179,\n",
       "  22035,\n",
       "  107054,\n",
       "  112269,\n",
       "  13,\n",
       "  83719,\n",
       "  100654,\n",
       "  84415,\n",
       "  101347,\n",
       "  34804,\n",
       "  220,\n",
       "  19,\n",
       "  100392,\n",
       "  22817,\n",
       "  63718,\n",
       "  121066,\n",
       "  95415,\n",
       "  19954,\n",
       "  103745,\n",
       "  116273,\n",
       "  69697,\n",
       "  112,\n",
       "  57575,\n",
       "  78453,\n",
       "  55430,\n",
       "  109791,\n",
       "  35495,\n",
       "  102888,\n",
       "  101347,\n",
       "  49085,\n",
       "  120893,\n",
       "  32179,\n",
       "  106872,\n",
       "  102077,\n",
       "  11,\n",
       "  111323,\n",
       "  19954,\n",
       "  55925,\n",
       "  106687,\n",
       "  75908,\n",
       "  60798,\n",
       "  116039,\n",
       "  101264,\n",
       "  103211,\n",
       "  13,\n",
       "  55925,\n",
       "  109055,\n",
       "  19954,\n",
       "  108154,\n",
       "  58083,\n",
       "  108733,\n",
       "  60798,\n",
       "  81673,\n",
       "  75908,\n",
       "  104911,\n",
       "  44005,\n",
       "  103315,\n",
       "  69697,\n",
       "  250,\n",
       "  103272,\n",
       "  30446,\n",
       "  123486,\n",
       "  107123,\n",
       "  33931,\n",
       "  101360,\n",
       "  120878,\n",
       "  48424,\n",
       "  13094,\n",
       "  101464,\n",
       "  109018,\n",
       "  122787,\n",
       "  24140,\n",
       "  44005,\n",
       "  78102,\n",
       "  101968,\n",
       "  55430,\n",
       "  24486,\n",
       "  106243,\n",
       "  18359,\n",
       "  64432,\n",
       "  103222,\n",
       "  230,\n",
       "  103170,\n",
       "  11,\n",
       "  108542,\n",
       "  82818,\n",
       "  113222,\n",
       "  250,\n",
       "  120376,\n",
       "  13094,\n",
       "  23955,\n",
       "  46230,\n",
       "  94,\n",
       "  18359,\n",
       "  16633,\n",
       "  232,\n",
       "  58901,\n",
       "  62398,\n",
       "  105512,\n",
       "  116548,\n",
       "  20565,\n",
       "  105365,\n",
       "  101787,\n",
       "  107621,\n",
       "  49085,\n",
       "  91786,\n",
       "  13,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [100848,\n",
       "  104762,\n",
       "  108047,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e4cb886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?\n",
      "문맥: 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_dataset[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db9cb04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?\n",
      "문맥: 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_dataset[0]['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b2dc3",
   "metadata": {},
   "source": [
    "##### 3. 모델 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2d726d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,                                    # 모델 학습 시 사용되는 모듈의 수, 저차원의 rank 수\n",
    "    lora_alpha=32,                          # lora_alpha 값이 클수록 모델 학습 시 사용되는 모듈의 수가 많아짐\n",
    "    lora_dropout=0.1,                       # 드롭아웃 비율, 과적합 방지\n",
    "    bias='none',                            # 편향 사용 여부\n",
    "    target_modules=['q_proj', 'v_proj'],    # 학습할 모듈 지정, 대상이 될 모듈 지정 ('q_proj': 쿼리 프로젝션, 'v_proj': 값 프로젝션)\n",
    "    task_type='CAUSAL_LM'                   # 학습 타입 지정 (Causal Language Model: 예측 모델)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe472c7",
   "metadata": {},
   "source": [
    "##### target_modules\n",
    "|  옵션  | 설명 |\n",
    "|:---------------------:|:--------------------------------------------------------------|\n",
    "| `'q_proj'`            | Attention의 Query projection. 주로 효율과 성능의 균형을 위해 기본적으로 선택됨. |\n",
    "| `'v_proj'`            | Attention의 Value projection. 모델 성능 최적화 시, 특히 대형 모델에 권장됨. |\n",
    "| `'k_proj'`            | Attention의 Key projection. Value projection과 함께 성능 최적화 및 대형 모델에 적합. |\n",
    "| `'o_proj'`            | Attention Output projection. 성능 향상을 위해 대형 모델에서 사용 가능. |\n",
    "| `'gate_proj'`         | 게이트 프로젝션. 게이트 구조와 결합하여 추가적인 성능 개선 가능. |\n",
    "| `'down_proj'`         | 다운 프로젝션. 차원 축소 등 특정 구조에서 성능 및 효율 개선을 위해 활용. |\n",
    "| `'up_proj'`           | 업 프로젝션. 차원 확장 구조에서 성능 향상 혹은 모델 구조 의도에 따라 적용. |\n",
    "\n",
    "---\n",
    "##### task_type\n",
    "|  옵션      | 설명 |\n",
    "|:--------------------:|:--------------------------------------------------------------|\n",
    "| `'CAUSAL_LM'`        | 입력 기반 다음 토큰 예측 (언어모델링/텍스트 생성) 태스크에 사용. |\n",
    "| `'SEQ_2_SEQ_LM'`     | 입력 시퀀스 → 출력 시퀀스 생성(예: 번역, 요약 등) 태스크에 사용.      |\n",
    "| `'SEQ_CLS'`          | 시퀀스 전체 분류(예: 감정 분류, 스팸 분류 등) 태스크에 사용.          |\n",
    "| `'TOKEN_CLS'`        | 각 토큰별 클래스를 예측(예: 개체명 인식) 태스크에 사용.              |\n",
    "| `'QUESTION_ANSWERING'` | 질문+문맥이 주어질 때 답 예측(질의응답) 태스크에 사용.           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa86aa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "# 모델에 lora 적용\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "# 입력 데이터에 대한 그래디언트 계산 활성화\n",
    "model.enable_input_require_grads()\n",
    "# 그래디언트 체크포인팅 활성화\n",
    "model.gradient_checkpointing_enable()\n",
    "# 학습 가능한 파라미터 출력\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a176d5",
   "metadata": {},
   "source": [
    "```python\n",
    "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n",
    "```\n",
    "\n",
    "위 결과는 현재 로라(LoRA) 방법으로 학습하는 파라미터(가중치)의 개수와 전체 모델 파라미터 중 학습 가능 파라미터가 차지하는 비율을 보여줍니다.\n",
    "\n",
    "| 항목                | 값 (예시)           | 설명                                                                              |\n",
    "|---------------------|---------------------|-----------------------------------------------------------------------------------|\n",
    "| trainable params    | 3,407,872           | 실제로 학습이 진행되는 파라미터 개수.<br>약 340만 개만 업데이트 및 미세조정 됨.      |\n",
    "| all params          | 8,033,669,120       | 전체 사전학습 모델의 파라미터 총 개수.<br>대부분은 고정(freeze)되어 있음.           |\n",
    "| trainable %         | 0.0424%             | 전체 파라미터 중 실제 학습이 적용되는 비율.<br>아주 작은 일부만 미세조정 됨.         |\n",
    "\n",
    "**설명**\n",
    "- LoRA와 같은 방식은 전체 대용량 파라미터 중 극히 일부만 효율적으로 학습하여, 연산 자원과 메모리를 크게 절약할 수 있음.\n",
    "- 우리 코드에서는 학습률(learning rate)을 1%로 주어, 학습 대상 파라미터가 빠르게 수렴하도록 설정함.\n",
    "- LoRA 등 파라미터 효율 방식은 학습률을 비교적 높게 줄 때 효과적이며, 빠른 실험과 수렴에 유리함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3ac3293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataCollatorForSeq2Seq 초기화 → 데이터 셋 전처리 과정에서 사용되는 클래스\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a256e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./q_lora_korqa',\n",
    "    save_strategy='epoch',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8, # batch size가 작을때 누산해서 학습을 효과적으로 진행하는 방법\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,             # 허깅페이스 허락 없이 푸시 하면 오류 발생\n",
    "    report_to='none'               # report 없이 진행\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d46dc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_24872\\301690771.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 훈련 프로세스 초기화\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    # tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec514135",
   "metadata": {},
   "source": [
    "##### 4. 모델 학습(Fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e2fd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 0}.\n",
      "c:\\Users\\Playdata\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# 훈련 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31423c1",
   "metadata": {},
   "source": [
    "##### 5. 학습된 모델 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b30a8f0",
   "metadata": {},
   "source": [
    "(1) 모델 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7400ca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정 저장\n",
    "from transformers import AutoConfig\n",
    "\n",
    "trained_model_path = './q_lora_korqa/checkpoint-1888'\n",
    "# 사전학습된 모델 객체의 정보를 불러온다. 기반 모델의 정보를 불러와서 fine tuning 된 모델 경로에 저장해준다.\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.save_pretrained(trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110bfe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이스 모델 학습 결과\n",
    "from peft import PeftModel\n",
    "from transformers import pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# --- 중요 ---\n",
    "# 훈련 시 사용했던 4-bit 양자화 설정을 *그대로* 다시 정의해야 합니다.\n",
    "# (노트북 세션이 재시작되었다면 quant_config 객체가 없을 수 있으므로)\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 모델을 로드할 때, 훈련 시와 *동일하게* quantization_config를 전달합니다.\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config,  # <-- *이것이 핵심입니다*\n",
    "    dtype=torch.bfloat16,             # 'torch_dtype' 대신 최신 'dtype' 사용\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# 이제 4-bit 베이스 모델에 4-bit용으로 훈련된 어댑터를 로드합니다.\n",
    "model = PeftModel.from_pretrained(base_model, trained_model_path)\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e05e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d99f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?'\n",
    "context = '1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.'\n",
    "\n",
    "input_text = f\"질문: {question}\\n문맥: {context}\"\n",
    "\n",
    "output = qa_pipeline(input_text, max_new_tokens=50, temperature=0.2, top_p=0.8)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e477cd",
   "metadata": {},
   "source": [
    "(2) HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca4b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./q_lora_korqa')\n",
    "tokenizer.save_pretrained('./q_lora_korqa')\n",
    "\n",
    "model.push_to_hub('vfxpedia/q_lora_korqa')\n",
    "tokenizer.push_to_hub('vfxpedia/q_lora_korqa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8049fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace 에서 Load해서 사용 (Base모델, 어뎁터 각각 모드)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"NCSOFT/Llama-VARCO-8B-Instruct\",\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# LoRa 어뎁터 로드\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    'vfxpedia/q_lora_korqa'\n",
    ")\n",
    "\n",
    "# tokenizer 로드\n",
    "toekenizer = AutoTokenizer.from_pretrained('vfxpedia/q_lora_korqa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0257c8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?', return_tensors='pt').to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa5ee60",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
