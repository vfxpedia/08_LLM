{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82353ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu128)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu128)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f7779a-60f6-41c5-a9d1-2c35ff7a1608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting openai\n",
      "  Using cached openai-2.7.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu128)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Using cached jiter-0.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Using cached pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Using cached openai-2.7.1-py3-none-any.whl (1.0 MB)\n",
      "Using cached jiter-0.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (361 kB)\n",
      "Using cached pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
      "Using cached pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, tqdm, safetensors, regex, python-dotenv, pydantic-core, jiter, hf-xet, annotated-types, pydantic, huggingface-hub, tokenizers, openai, transformers, accelerate\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15\u001b[0m [accelerate]5\u001b[0m [accelerate]s]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.11.0 annotated-types-0.7.0 hf-xet-1.2.0 huggingface-hub-0.36.0 jiter-0.12.0 openai-2.7.1 pydantic-2.12.4 pydantic-core-2.41.5 python-dotenv-1.2.1 regex-2025.11.3 safetensors-0.6.2 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.1 typing-inspection-0.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv transformers accelerate openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "573e339d-771a-40a2-8e7a-b922bcec7879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hf_transfer\n",
      "  Using cached hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Using cached hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Installing collected packages: hf_transfer\n",
      "Successfully installed hf_transfer-0.1.9\n"
     ]
    }
   ],
   "source": [
    "!pip install hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5abed39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf_xet in /usr/local/lib/python3.12/dist-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25a29693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    " !pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dac11ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.12/dist-packages (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (9.6.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.12/dist-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d8800fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -n base -c conda-forge widgetsnbextension\n",
    "# conda install -n llm_env -c conda-forge ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f2553b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43c55fb6-c243-444d-a015-70d2a340a5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 5090'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__ # '2.8.0+cu128'\n",
    "torch.cuda.is_available() # True\n",
    "torch.cuda.get_device_name(0) # 'NVIDIA A40'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aa966d",
   "metadata": {},
   "source": [
    "---\n",
    "01. sllm 사용해서 text-generation 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42532594-80bc-452e-b5de-60ee14c0b7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32603c26a0749a589778acb56c43593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# LLM 모델, sLLM 모델 성능 비교 (Text-Generation)\n",
    "sllm_model_name = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(sllm_model_name)\n",
    "sllm = AutoModelForCausalLM.from_pretrained(sllm_model_name, device_map='auto') # 사용가능한 gpu 자동할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96952db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Please explain the theory of relativity.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0c24e9c-a11e-4846-9a64-437f96214535",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors='pt').to(sllm.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c78e2b2-0101-46f6-a1cf-c2371e4a145d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = sllm.generate(**inputs, max_length=500)\n",
    "sllm_res = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d26cee99-f852-48f2-ac55-e6507cc5d6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please explain the theory of relativity. This is a complex topic, but I'll try to break it down in simple terms.\n",
      "\n",
      "The theory of relativity, developed by Albert Einstein, consists of two main components: special relativity and general relativity.\n",
      "\n",
      "**Special Relativity (1905)**\n",
      "\n",
      "Special relativity posits that the laws of physics are the same for all observers in uniform motion relative to one another. This theory challenged the long-held notion of absolute time and space.\n",
      "\n",
      "Key concepts:\n",
      "\n",
      "1. **The speed of light is constant**: Regardless of the motion of the observer or the source of light, the speed of light is always the same (approximately 299,792,458 meters per second).\n",
      "2. **Time dilation**: Time appears to pass slower for an observer in motion relative to a stationary observer. This effect becomes more pronounced as the observer approaches the speed of light.\n",
      "3. **Length contraction**: Objects appear shorter to an observer in motion relative to a stationary observer.\n",
      "4. **Relativity of simultaneity**: Two events that are simultaneous for one observer may not be simultaneous for another observer in a different state of motion.\n",
      "\n",
      "**General Relativity (1915)**\n",
      "\n",
      "General relativity builds upon special relativity and introduces the concept of gravity as a curvature of spacetime caused by massive objects.\n",
      "\n",
      "Key concepts:\n",
      "\n",
      "1. **Spacetime**: Einstein combined space and time into a single entity called spacetime, which is flexible and dynamic.\n",
      "2. **Gravity as curvature**: Massive objects warp spacetime, causing other objects to move along curved trajectories, which we experience as gravity.\n",
      "3. **Equivalence principle**: The effects of gravity are equivalent to the effects of acceleration. An observer in a gravitational field will experience the same effects as an observer who is accelerating.\n",
      "4. **Geodesics**: The shortest path through spacetime, which is a straight line in flat spacetime but a curved line in the presence of gravity.\n",
      "\n",
      "**Implications and Consequences**\n",
      "\n",
      "The theory of relativity has far-reaching implications and consequences, including:\n",
      "\n",
      "1. **Time and space are relative**: Time and space are not absolute, but depend on the observer's frame of reference.\n",
      "2. **The speed of light is the universal speed limit**: No object can reach or exceed the speed of light.\n",
      "3. **Gravity is not a force**: Gravity is a consequence of the curvature of spacetime, not a force that acts between objects.\n",
      "4. **Black holes and wormholes\n"
     ]
    }
   ],
   "source": [
    "print(sllm_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2f7fcd-160b-4613-8db3-f86b7b9887ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b9eb5f0-029c-43b8-bf2e-0051a7a39920",
   "metadata": {},
   "source": [
    "---\n",
    "02. llm 사용해서 text-generation 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afab3ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5bd6d34-a9ce-4135-a7d8-7d7043658462",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Please explain the theory of relativity.\"\n",
    "\n",
    "gpt_res = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "762a57b2-cf99-4f0e-b60d-7ccfc4aef98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The theory of relativity, developed by Albert Einstein in the early 20th century, fundamentally changed our understanding of space, time, and gravity. It consists of two main theories: special relativity and general relativity.\n",
      "\n",
      "### Special Relativity (1905)\n",
      "Special relativity addresses the behavior of objects moving at constant speeds, particularly those approaching the speed of light. Its key postulates are:\n",
      "\n",
      "1. **Relativity of Simultaneity**: Events that are simultaneous from one observer's perspective may not be simultaneous from another's, depending on their relative motion.\n",
      "   \n",
      "2. **Constant Speed of Light**: The speed of light in a vacuum is constant (approximately 299,792 kilometers per second) for all observers, regardless of their motion relative to the light source.\n",
      "\n",
      "Key consequences of special relativity include:\n",
      "\n",
      "- **Time Dilation**: Time can pass at different rates for observers in different states of motion. For example, a moving clock ticks more slowly than a stationary clock from the perspective of the stationary observer.\n",
      "  \n",
      "- **Length Contraction**: Objects in motion will appear shorter along the direction of motion relative to a stationary observer.\n",
      "\n",
      "- **Mass-Energy Equivalence**: This is famously encapsulated in the equation \\(E=mc^2\\), which states that mass and energy are interchangeable; a small amount of mass can be converted into a large amount of energy.\n",
      "\n",
      "### General Relativity (1915)\n",
      "General relativity extends the principles of special relativity to include acceleration and gravity. Its core idea is:\n",
      "\n",
      "- **Gravity as Curvature of Spacetime**: Massive objects like planets and stars curve the fabric of spacetime around them. This curvature affects the motion of other objects, which move along paths determined by this curvature. Rather than thinking of gravity as a force acting at a distance, general relativity describes it as the effect of spacetime geometry.\n",
      "\n",
      "Key implications of general relativity include:\n",
      "\n",
      "- **Gravitational Time Dilation**: Time runs slower in stronger gravitational fields. For example, a clock close to a massive object (like Earth) ticks more slowly than a clock farther away.\n",
      "  \n",
      "- **Bending of Light**: Light travels along the curved paths in warped spacetime. This means that light from distant stars is bent when it passes near a massive object (an effect confirmed during solar eclipses).\n",
      "\n",
      "- **Black Holes**: Regions of spacetime where gravity is so intense that nothing, not even light, can escape from them.\n",
      "\n",
      "General relativity has been validated through numerous experiments and observations, profoundly influencing fields ranging from astrophysics to cosmology. Together, both theories of relativity have reshaped our understanding of the universe's structure and the fundamental nature of reality.\n"
     ]
    }
   ],
   "source": [
    "print(gpt_res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d8d135c-921b-492f-831a-a9fa0e25ea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt0 = \"초심자가 점진적으로 학습해서 AI 개발자 혹은 MLOps 전문가가 되기 위해서는 단계별로 어떻게 학습을 해야해? 너무 공부하기가 어려워\"\n",
    "gpt_res = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt0}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14a0ef93-d352-4142-8466-a40d23c44a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI 개발자 또는 MLOps 전문가가 되기 위한 단계별 학습 방법은 다음과 같습니다. 각 단계를 따라가며 천천히 학습해 나가면 점진적으로 필요한 지식을 쌓을 수 있습니다.\n",
      "\n",
      "### 1. 기초 지식 쌓기\n",
      "\n",
      "#### 언어 및 기본 프로그래밍 이해\n",
      "- **Python**: AI와 ML 분야에서 가장 많이 사용되는 언어이므로 Python을 배우는 것이 중요합니다. 기본 문법, 데이터 구조, 제어문 등을 익히세요.\n",
      "- **온라인 강좌**: Codecademy, Coursera, edX 등에서 Python 기본 과정을 듣기.\n",
      "\n",
      "#### 수학 & 통계\n",
      "- **기초 수학**: 선형 대수학, 미적분, 확률 및 통계 등을 공부하세요. AI와 머신러닝의 기본 이해에 필요합니다.\n",
      "- **추천 자료**: Khan Academy, MIT OpenCourseWare에서 관련 자료 찾기.\n",
      "\n",
      "### 2. 머신러닝 기초\n",
      "\n",
      "#### 머신러닝 개념 이해\n",
      "- **기본 개념**: 지도 학습, 비지도 학습, 강화 학습 등 기본 개념을 이해합니다.\n",
      "- **추천 강좌**: Andrew Ng의 머신러닝 강의(Coursera) 수강.\n",
      "\n",
      "#### 타이타닉 데이터셋 같은 간단한 프로젝트\n",
      "- **실습**: Kaggle의 쉬운 데이터셋을 이용해 기본적인 머신러닝 모델을 만들어보세요.\n",
      "\n",
      "### 3. 데이터 처리 및 분석\n",
      "\n",
      "#### Pandas 및 Numpy 사용법\n",
      "- 데이터 전처리와 분석을 위한 기본 패키지인 Pandas와 Numpy를 배우세요.\n",
      "- **프로젝트**: 공공 데이터셋을 사용해 데이터 분석 실습하기.\n",
      "\n",
      "### 4. 심화 머신러닝 및 딥러닝\n",
      "\n",
      "#### 딥러닝\n",
      "- **기초 이론**: 신경망의 구조, CNN, RNN 등 딥러닝의 기초 개념을 학습하세요.\n",
      "- **추천 강좌**: Deep Learning Specialization (Coursera) 수강.\n",
      "\n",
      "#### 프레임워크 사용\n",
      "- **TensorFlow 또는 PyTorch**: 딥러닝 프레임워크의 사용법을 익히고 실제 프로젝트를 통해 적용합니다.\n",
      "\n",
      "### 5. MLOps 이해하기\n",
      "\n",
      "#### MLOps 기본 개념\n",
      "- MLOps란 무엇이며 왜 필요한지에 대한 개념을 이해합니다.\n",
      "- **CI/CD, 모델 배포, 모니터링** 등을 포함하는 MLOps 프로세스에 대해 배워야 합니다.\n",
      "\n",
      "#### 실습 프로젝트\n",
      "- 모델 학습, 배포, 모니터링 등을 포함한 전체 과정을 직접 경험해보세요. Docker와 Kubernetes 같은 도구도 함께 배우면 좋습니다.\n",
      "\n",
      "### 6. 커뮤니티 참여 및 네트워크 형성\n",
      "\n",
      "- **포럼 참여**: Kaggle, GitHub, Stack Overflow 등의 온라인 커뮤니티에 참여하십시오.\n",
      "- **오픈소스 프로젝트 기여**: GitHub에서 오픈 소스 프로젝트에 기여하면서 실무 경험을 쌓으세요.\n",
      "\n",
      "### 7. 계속 배우고 발전하기\n",
      "\n",
      "- **최신 기술 트렌드**: 지속적으로 AI 및 MLOps 관련 최신 기술과 연구를 학습하세요. 블로그, 논문, 컨퍼런스 등을 통해 새로운 정보를 접하고, 관련 토픽에 대한 지식을 심화합니다.\n",
      "\n",
      "### 추가 팁\n",
      "\n",
      "1. **꾸준함**: 매일 조금씩이라도 공부하는 것이 중요합니다.\n",
      "2. **메타인지**: 내가 무엇을 잘 알고 무엇을 모르는지 스스로 평가하고 부족한 부분에 초점을 맞추세요.\n",
      "3. **멘토 찾기**: 도움을 줄 수 있는 멘토나 동료와 함께 학습하면 더 많은 동기부여가 될 수 있습니다.\n",
      "\n",
      "AI 및 MLOps 분야는 방대하지만, 꾸준한 노력과 실습이 가장 중요합니다. 시간이 걸리더라도 목표를 향해 천천히 나아가길 바랍니다!\n"
     ]
    }
   ],
   "source": [
    "print(gpt_res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b1997c-6fd4-4a4f-a818-5f3e615d4e68",
   "metadata": {},
   "source": [
    "---\n",
    "sllm Llama 모델로 확인해보기(답변 비교)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "129dd135-3f0f-450a-8848-02d97fde40b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please explain the theory of relativity.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58e3c220-6943-44bd-81ba-28300cc8b9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4561c529873c4a46878bfac37c618c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# LLM 모델, sLLM 모델 성능 비교 (Text-Generation)\n",
    "\n",
    "sllm_model_name = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(sllm_model_name)\n",
    "sllm = AutoModelForCausalLM.from_pretrained(sllm_model_name, device_map='auto') # device_mpa 'auto': 사용가능한 gpu 자동할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8b02fac-ee82-4aef-b7b2-e299f284e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors='pt').to(sllm.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4b3bdb1-e291-4ede-b12c-daaf32f5756a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = sllm.generate(**inputs, max_length=500)\n",
    "sllm_res = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61916845-be8e-4fa4-83dc-f15f72dbad63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please explain the theory of relativity. Albert Einstein's theory of relativity, which was introduced in 1905, consists of two main components: special relativity and general relativity. Special relativity describes the behavior of objects at high speeds, while general relativity describes the behavior of objects in the presence of gravity.\n",
      "## Step 1: Introduce the theory of relativity\n",
      "The theory of relativity, developed by Albert Einstein, is a fundamental concept in modern physics that has revolutionized our understanding of space and time.\n",
      "\n",
      "## Step 2: Explain special relativity\n",
      "Special relativity posits that the laws of physics are the same for all observers in uniform motion relative to one another. This theory challenged the long-held notion of absolute time and space. According to special relativity, time and space are relative, and their measurement depends on the observer's frame of reference.\n",
      "\n",
      "## Step 3: Describe the famous equation E=mc^2\n",
      "One of the most famous equations derived from special relativity is E=mc^2, which states that energy (E) is equal to mass (m) times the speed of light (c) squared. This equation shows that mass and energy are interchangeable, and that a small amount of mass can be converted into a large amount of energy, and vice versa.\n",
      "\n",
      "## Step 4: Explain general relativity\n",
      "General relativity builds upon special relativity and introduces the concept of gravity as a curvature of spacetime caused by the presence of mass and energy. According to general relativity, the curvature of spacetime around a massive object such as the Earth causes objects to fall towards the center of the Earth, which we experience as gravity.\n",
      "\n",
      "## Step 5: Describe the concept of spacetime\n",
      "Spacetime is a four-dimensional fabric that combines space and time. According to general relativity, the curvature of spacetime is a fundamental aspect of the universe, and it plays a crucial role in the behavior of objects with mass and energy.\n",
      "\n",
      "## Step 6: Summarize the key components of the theory of relativity\n",
      "The theory of relativity consists of two main components: special relativity and general relativity. Special relativity describes the behavior of objects at high speeds, while general relativity describes the behavior of objects in the presence of gravity. The theory of relativity has had a profound impact on our understanding of the universe and has led to numerous breakthroughs in fields such as astrophysics and cosmology\n"
     ]
    }
   ],
   "source": [
    "print(sllm_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda35de8-32e8-405e-89a5-5f222056f2f3",
   "metadata": {},
   "source": [
    "---\n",
    "sLLM-LLM 모델 속도 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1d1195c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926050e7131e4bde840aa14ce21ccb47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "sllm_model_name = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(sllm_model_name)\n",
    "sllm = AutoModelForCausalLM.from_pretrained(sllm_model_name, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69f709ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd821c9522241f2a537267c58ce472b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7879dae5e24a8dafe026cffde9180c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe16fc164a704edba23e40a8a1c2877a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce42a0227d6443c68273c4b305040833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ef756714844f75896c2992373f7252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5280ec101194482b7f0d68a72e6aa3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9303fc7ad945359ad299f4837a2eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4400b44f8b4c08b19e2e9b19e93c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "llm_model_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "llm = AutoModelForCausalLM.from_pretrained(llm_model_name, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "427e2b2e-42aa-42e1-83ba-61fe0b28354e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please explain the theory of relativity.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4eb60599-544b-40d7-98e4-cada13fd2107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu128)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4fa85412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please explain the theory of relativity. Albert Einstein's groundbreaking theory of relativity revolutionized our understanding of space and time. The theory consists of two main components: special relativity and general relativity.\n",
      "\n",
      "## Step 1: Introduce the Theory of Relativity\n",
      "The theory of relativity, developed by Albert Einstein, is a fundamental concept in modern physics that challenged long-held notions of space and time. The theory is composed of two main parts: special relativity and general relativity.\n",
      "\n",
      "## Step 2: Special Relativity\n",
      "Special relativity, introduced in 1905, posits that the laws of physics are the same for all observers in uniform motion relative to one another. This theory challenged the long-held notion of absolute time and space. The core principles of special relativity include:\n",
      "\n",
      "* The laws of physics are invariant under Lorentz transformations\n",
      "* The speed of light is constant and unchanging for all observers\n",
      "* Time dilation and length contraction occur when objects move at high speeds\n",
      "\n",
      "## Step 3: General Relativity\n",
      "General relativity, introduced in 1915, builds upon the principles of special relativity and introduces the concept of gravity as a curvature of spacetime caused by the presence of mass and energy. According to general relativity:\n",
      "\n",
      "* The curvature of spacetime around a massive object such as the Earth causes objects to fall towards the center of the Earth\n",
      "* The equivalence principle states that the effects of gravity are equivalent to the effects of acceleration\n",
      "* Gravitational time dilation and gravitational redshift occur due to the curvature of spacetime\n",
      "\n",
      "## Step 4: Key Implications of the Theory of Relativity\n",
      "The theory of relativity has far-reaching implications for our understanding of the universe, including:\n",
      "\n",
      "* The speed of light is the universal speed limit\n",
      "* Time and space are relative, and their measurement depends on the observer's frame of reference\n",
      "* Gravity is not a force, but rather a curvature of spacetime caused by mass and energy\n",
      "* The universe is expanding, and the expansion is accelerating\n",
      "\n",
      "The final answer is: $\\boxed{There is no numerical answer to this problem, as it is a descriptive explanation of the theory of relativity.}$\n",
      "\n",
      "Note: I've followed the format you requested, but since there is no numerical answer, I couldn't provide a boxed answer. If you'd like, I can rephrase the final step to better fit the format. Let me know!\n",
      "시간: 13.16초\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.96 GiB. GPU 0 has a total capacity of 31.37 GiB of which 973.81 MiB is free. Including non-PyTorch memory, this process has 30.41 GiB memory in use. Of the allocated memory 27.84 GiB is allocated by PyTorch, and 1.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m inputs = tokenizer(prompt, return_tensors=\u001b[33m'\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m'\u001b[39m).to(model.device)\n\u001b[32m      8\u001b[39m start_time = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m end_time = time.time()\n\u001b[32m     12\u001b[39m model_res = tokenizer.decode(outputs[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py:473\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[32m    472\u001b[39m slice_indices = \u001b[38;5;28mslice\u001b[39m(-logits_to_keep, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits_to_keep, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m logits_to_keep\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py:170\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_forward\u001b[39m(module, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     args, kwargs = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_hf_hook\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module._hf_hook.no_grad:\n\u001b[32m    172\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py:360\u001b[39m, in \u001b[36mAlignDevicesHook.pre_forward\u001b[39m\u001b[34m(self, module, *args, **kwargs)\u001b[39m\n\u001b[32m    352\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    353\u001b[39m             value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    354\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tied_params_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    355\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m value.data_ptr() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tied_params_map\n\u001b[32m    356\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.execution_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tied_params_map[value.data_ptr()]\n\u001b[32m    357\u001b[39m         ):\n\u001b[32m    358\u001b[39m             \u001b[38;5;28mself\u001b[39m.tied_pointers_to_remove.add((value.data_ptr(), \u001b[38;5;28mself\u001b[39m.execution_device))\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m         \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m.execution_device), send_to_device(\n\u001b[32m    370\u001b[39m     kwargs, \u001b[38;5;28mself\u001b[39m.execution_device, skip_keys=\u001b[38;5;28mself\u001b[39m.skip_keys\n\u001b[32m    371\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/modeling.py:343\u001b[39m, in \u001b[36mset_module_tensor_to_device\u001b[39m\u001b[34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map, non_blocking, clear_cache)\u001b[39m\n\u001b[32m    341\u001b[39m             module._parameters[tensor_name] = param_cls(new_value, requires_grad=old_value.requires_grad)\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     new_value = \u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    345\u001b[39m     new_value = torch.tensor(value, device=device)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 0 has a total capacity of 31.37 GiB of which 973.81 MiB is free. Including non-PyTorch memory, this process has 30.41 GiB memory in use. Of the allocated memory 27.84 GiB is allocated by PyTorch, and 1.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "models = [sllm, llm]\n",
    "\n",
    "for model in models:\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    outputs = model.generate(**inputs, max_length=500)\n",
    "    end_time = time.time()\n",
    "\n",
    "    model_res = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(model_res)\n",
    "    print(f'시간: {end_time - start_time:.2f}초')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9c0eab",
   "metadata": {},
   "source": [
    "### 응답 유지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8eff1696",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = \"I am planning a trip to Japan. What are the best cities to visit?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "15fe9d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "inputs2 = tokenizer(prompt2, return_tensors='pt').to(sllm.device)\n",
    "outputs2 = sllm.generate(**inputs2, max_length=500)\n",
    "response2 = tokenizer.decode(outputs2[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "89278a56-79af-4bb3-b43a-fb2ef8c270b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What local food should I try in those cities??\n",
      "Here are some local food recommendations for some major cities in the US:\n",
      "**New York City, NY**\n",
      "* **Pizza**: Lombardi's, Joe's Pizza, or Patsy's Pizzeria (try a classic slice or pie)\n",
      "* **Bagels**: Russ & Daughters Cafe, Ess-a-Bagel, or H&H Bagels (try a classic everything bagel with cream cheese)\n",
      "* **Hot Dogs**: Gray's Papaya or Papaya King (try a classic New York-style hot dog with sauerkraut and mustard)\n",
      "* **Knish**: Yonah Schimmel's Knish Bakery (try a classic potato knish)\n",
      "* **Dessert**: Lombardi's or Juniors (try a classic cheesecake or cannoli)\n",
      "\n",
      "**Los Angeles, CA**\n",
      "* **Tacos**: Guerrilla Tacos, Carnitas El Momo, or Guelaguetza (try a Baja-style taco or a traditional Oaxacan taco)\n",
      "* **Avocado Toast**: Sqirl or The Apple Pan (try a classic avocado toast with cherry tomatoes and feta)\n",
      "* **In-N-Out Burger**: Try a classic Double-Double with fries\n",
      "* **Fresh Fruit**: Santa Monica Farmers Market or Venice Beach Farmers Market (try a fresh fruit smoothie or a fruit salad)\n",
      "* **Dessert**: The Original Farmers Market or The Creamery (try a classic ice cream sandwich or a fresh fruit tart)\n",
      "\n",
      "**Chicago, IL**\n",
      "* **Deep-Dish Pizza**: Lou Malnati's or Pizzeria Uno (try a classic deep-dish pizza with sausage or mushroom)\n",
      "* **Hot Dogs**: Portillo's or Alinea (try a classic Chicago-style hot dog with relish and sport peppers)\n",
      "* **Italian Beef Sandwich**: Al's Beef or Mr. Beef (try a classic Italian beef sandwich with giardiniera)\n",
      "* **Polish Sausage**: Gene's Sausage Shop or Portillo's (try a classic Polish sausage with peppers and onions)\n",
      "* **Dessert**: Lou Malnati's or Gino's East (try a classic cheesecake or a chocolate cake)\n",
      "\n",
      "**San Francisco, CA**\n",
      "* **Sourdough Bread**: Boudin Bakery or Acme Bread Company (try a classic sourdough bread with butter or cheese)\n",
      "* **Fish Tacos**: La Taqueria or El Farol\n"
     ]
    }
   ],
   "source": [
    "prompt2 = 'What local food should I try in those cities?'\n",
    "\n",
    "inputs2 = tokenizer(prompt2, return_tensors='pt').to(sllm.device)\n",
    "outputs2 = sllm.generate(**inputs2, max_length=500)\n",
    "response2 = tokenizer.decode(outputs2[0], skip_special_tokens=True)\n",
    "\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44da666-339a-476c-814d-084b65c059ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "873d5a49-9283-42ae-a7df-ac13fc1afec0",
   "metadata": {},
   "source": [
    "### 다국어 처리 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f5241-e1ce-48c8-86b8-c809cde3a33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "multilingual_model = pipeline(\"text-generation\", model=\"google/gemma-2-9b-it\", device_map='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644da1fb-4836-4013-98bb-cfe687957408",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {\n",
    "    \"English\": \"The theory is composed of two main parts: special relativity and general relativity.\",\n",
    "    \"Korean\": \"이 이론은 특수 상대성 이론과 일반 상대성 이론이라는 두 가지 주요 부분으로 구성되어 있습니다.\",\n",
    "    \"Japanes\": \"この理論は特殊相対性理論と一般相対性理論という 2 つの主要な部分から構成されています。\"\n",
    "}\n",
    "\n",
    "for lang, prompt in prompts.items():\n",
    "    response = multilingual_model(prompt, max_length=300)\n",
    "    print(f\"\\n== {lang} ===\")\n",
    "    print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592fe0df-75ca-46f7-873e-199057f6d872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/gemma-2-9b-it\",\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cuda\",  # replace with \"mps\" to run on a Mac device\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you? Please, answer in pirate-speak.\"},\n",
    "]\n",
    "\n",
    "outputs = pipe(messages, max_new_tokens=256)\n",
    "assistant_response = outputs[0][\"generated_text\"][-1][\"content\"].strip()\n",
    "print(assistant_response)\n",
    "# Ahoy, matey! I be Gemma, a digital scallywag, a language-slingin' parrot of the digital seas. I be here to help ye with yer wordy woes, answer yer questions, and spin ye yarns of the digital world.  So, what be yer pleasure, eh? 🦜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6b4da27c-9a23-4fdb-8a83-344055acb895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9eb681a49634aae905c77a83e98219f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/857 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b48b7767a754bdbbc3857f28526e38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/39.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f0e01a4eae435d86e919a6177adf56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2576c52cc94323b1a7e2a11f9b7f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9612c7baac2f49b689fe111b59f15443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762c5f070b3a49f6ad6e044587948746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ddf1d1b0ad472b9212db7c3e7be192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d35be5756dd4fecb40c13b4221f9871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c48f5fd3574dff82ed2fbc843ce559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d452bc1036104f949bc05b8a54e8828e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3e096c6f334d398d4cb028e1a02846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a5666f58a845b1ac28926009f95358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== English ===\n",
      "That sentence means Einstein's theory of relativity has two big parts:\n",
      "\n",
      "1. **Special relativity:** Deals with how things move at very high speeds.\n",
      "2. **General relativity:** Explains gravity as a warping of space and time. \n",
      "\n",
      "\n",
      "Together, they make up Einstein's theory of relativity. \n",
      "\n",
      "\n",
      "\n",
      "== Korean ===\n",
      "This theory is made up of two main parts: special relativity and general relativity. \n",
      "\n",
      "\n",
      "== Japanese ===\n",
      "This theory is made up of two main parts: special relativity and general relativity. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# VRAM 절약 및 'auto' 권장\n",
    "multilingual_model = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"google/gemma-2-9b-it\", \n",
    "    device_map='auto', # 'cuda'보다 'auto'가 더 안전합니다.\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 2. [중요] 모델에게 '작업'을 지시하도록 프롬프트 수정\n",
    "base_prompts = {\n",
    "    \"English\": \"The theory is composed of two main parts: special relativity and general relativity.\",\n",
    "    \"Korean\": \"이 이론은 특수 상대성 이론과 일반 상대성 이론이라는 두 가지 주요 부분으로 구성되어 있습니다.\",\n",
    "    \"Japanese\": \"この理論は特殊相対性理論と一般相対性理論という 2 つの主要な部分から構成されています。\" \n",
    "    # 'Japanes' -> 'Japanese' (키 이름 수정)\n",
    "}\n",
    "\n",
    "# Gemma 2 채팅 템플릿에 맞춰 프롬프트 가공\n",
    "# \"이 문장을 간단히 설명해줘\" 라는 '지시'를 추가\n",
    "prompts_templated = {\n",
    "    lang: f\"<start_of_turn>user\\nExplain the following sentence simply:\\n\\\"{text}\\\"<start_of_turn>model\\n\"\n",
    "    for lang, text in base_prompts.items()\n",
    "}\n",
    "\n",
    "\n",
    "for lang, prompt in prompts_templated.items():\n",
    "    print(f\"\\n== {lang} ===\")\n",
    "    \n",
    "    response = multilingual_model(\n",
    "        prompt, \n",
    "        max_new_tokens=300,  # 3. 'max_length' 대신 'max_new_tokens' 사용\n",
    "        return_full_text=False # 4. 모델의 답변만 보도록 설정\n",
    "    )\n",
    "    \n",
    "    # 5. 괄호 ')' 추가\n",
    "    print(response[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
